AWSTemplateFormatVersion: "2010-09-09"
Description: Retrieves AWS Backup details across AWS organization
Parameters:
  DatabaseName:
    Type: String
    Description: Name of the Athena database to be created to hold lambda information
    Default: optimization_data
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket that exists or needs to be created to hold backup information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  DestinationBucketARN:
    Type: String
    Description: ARN of the S3 Bucket that exists or needs to be created to hold backup information
  ManagementRoleName:
    Type: String
    Description: The name of the IAM role that will be deployed in the management account which can retrieve AWS Organization data. KEEP THE SAME AS WHAT IS DEPLOYED INTO MANAGEMENT ACCOUNT
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: aws-backup
  GlueRoleARN:
    Type: String
    Description: Arn for the Glue Crawler role
  Schedule:
    Type: String
    Description: EventBridge Schedule to trigger the data collection
    Default: "rate(14 days)"
  ResourcePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  LambdaAnalyticsARN:
    Type: String
    Description: Arn of lambda for Analytics
  AccountCollectorLambdaARN:
    Type: String
    Description: Arn of the Account Collector Lambda
  StepFunctionTemplate:
    Type: String
    Description: JSON representation of common StepFunction template
  StepFunctionExecutionRoleARN:
    Type: String
    Description: Common role for Step Function execution
  SchedulerExecutionRoleARN:
    Type: String
    Description: Common role for module Scheduler execution

Outputs:
  StepFunctionARN:
    Description: ARN for the module's Step Function
    Value: !GetAtt ModuleStepFunction.Arn

Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${ResourcePrefix}${CFDataName}-LambdaRole"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Path: /
      Policies:
        - PolicyName: !Sub "${CFDataName}-ManagementAccount-LambdaRole"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: !Sub "arn:aws:iam::*:role/${ManagementRoleName}" # Need to assume a Read role in all Management accounts
        - PolicyName: "S3-Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:PutObject"
                  - "s3:GetObject"
                  - "s3:PutObjectAcl"
                Resource:
                  - !Sub "${DestinationBucketARN}/*"
              - Effect: "Allow"
                Action:
                  - "s3:ListBucket"
                Resource:
                  - !Sub "${DestinationBucketARN}"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ResourcePrefix}${CFDataName}-Lambda-Backup-Records'
      Description: !Sub "Lambda function to retrieve ${CFDataName}"
      Runtime: python3.10
      Architectures: [x86_64]
      Code:
        ZipFile: |
          import os
          import json
          import logging
          from datetime import date, timedelta, datetime

          import boto3
          from boto3.s3.transfer import S3Transfer

          logger = logging.getLogger()

          BUCKET = os.environ['BUCKET_NAME']
          ROLE = os.environ['ROLENAME']
          PREFIX = os.environ['PREFIX']
          CRAWLER_NAME = os.environ['CRAWLER_NAME']
          MANAGEMENT_ACCOUNT_IDS = os.environ['MANAGEMENT_ACCOUNT_IDS']

          def start_crawler():
              glue = boto3.client('glue')
              try:
                  glue.start_crawler(Name=CRAWLER_NAME)
              except Exception as exc:
                  logging.warning(exc)

          def store_data_to_s3(flattened_data, path):
              local_file = '/tmp/tmp.json'
              with open(local_file, 'w') as f:
                  f.write('\n'.join([json.dumps(result) for result in flattened_data]))
              if os.path.getsize(local_file) == 0:
                  logger.info(f"No data in file for {path}")
                  return
              s3client = boto3.client('s3')
              key = date.today().strftime(f"{path}/year=%Y/month=%m/day=%m/%Y-%m-%d.json")
              logger.debug(f"Uploading file {local_file} to {BUCKET}/{key}")
              S3Transfer(s3client).upload_file(local_file, BUCKET, key, extra_args={'ACL': 'bucket-owner-full-control'})
              logger.info(f'uploaded to s3://{BUCKET}/{key}')

          def get_paginated_results(client, func, object, params=None):
              return client.get_paginator(func).paginate(**(params or {})).build_full_result()[object]

          def flatten_results(results, parent_key='',sep='_'):
              flattened_results = {}
              for key,value in results.items():
                  new_key = parent_key + sep + key if parent_key else key
                  if isinstance(value, dict):
                      flattened_results.update(flatten_results(value, new_key, sep=sep))
                  elif isinstance(value, datetime):
                      flattened_results[new_key] = value.isoformat()
                  else:
                      flattened_results[new_key] = value
              return flattened_results

          def last_updated_date(s3_path):
              #Returns a list of existing backup jobs for an authenticated account for the last 30 days
              start_date = datetime.now().date() - timedelta(days=30)
              # Check the create time of objects in the S3 bucket
              contents = boto3.client('s3').get_paginator('list_objects_v2').paginate(Bucket=BUCKET, Prefix=s3_path).get('Contents', [])
              last_modified_date = get_last_modified_date(contents)
              if last_modified_date and last_modified_date >= start_date:
                  start_date = last_modified_date
              return start_date

          def get_last_modified_date(contents):
              last_modified_dates = [obj['LastModified'].date() for obj in contents]
              last_modified_dates_within_30_days = [date for date in last_modified_dates if date >= datetime.now().date() - timedelta(days=30)]
              return max(last_modified_dates_within_30_days or [None])


          def lambda_handler(event, context):
              total_count = 0
              for payer_id in [r.strip() for r in MANAGEMENT_ACCOUNT_IDS.split(',')]:
                  try:
                      creds = boto3.client('sts').assume_role(
                          RoleArn=ROLE_ARN,
                          RoleSessionName="cross_acct_lambda"
                      )['Credentials']
                      backup = boto3.client(
                          "backup",
                          "us-east-1",
                          aws_access_key_id=creds['AccessKeyId'],
                          aws_secret_access_key=creds['SecretAccessKey'],
                          aws_session_token=creds['SessionToken'],
                      )
                      for item in ['backup', 'copy', 'restore']:
                          s3_prefix = f'{PREFIX}/backup-{item}-jobs-data/payer_id={payer_id}'
                          start_date = last_updated_date(s3_prefix)
                          end_date = datetime.now().date()
                          data = get_paginated_results(
                              client=backup,
                              function=f'list_{item}_jobs'
                              object=f'{item.capitalize()}Jobs'
                              params={
                                  ByCompleteAfter=str(start_date),
                                  ByCompleteBefore=str(end_date),
                                  ByAccountId='*',
                              },
                          )
                          flattened_data = map(flatten_results, data)
                          store_data_to_s3(flattened_data, f'{PREFIX}/backup-jobs-data/payer_id={payer_id}')
                  except Exception as exc:
                      print(payer_id, exc)
              return "Successful"

      Handler: "index.lambda_handler"
      MemorySize: 2688
      Timeout: 600
      Role: !GetAtt LambdaRole.Arn
      Environment:
        Variables:
          BUCKET_NAME: !Ref DestinationBucket
          PREFIX: !Ref CFDataName
          ROLENAME: !Ref ManagementRoleName
          CRAWLER_NAME: !Ref Crawler
          MANAGEMENT_ACCOUNT_IDS: !Ref AWS::AccountId
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"


  Crawler: #FIXME - need a crawler per path
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Sub '${ResourcePrefix}${CFDataName}-Crawler'
      Role: !Ref GlueRoleARN
      DatabaseName: !Ref DatabaseName
      Targets:
        S3Targets:
          - Path: !Sub "s3://${DestinationBucket}/${CFDataName}/${CFDataName}-data/"

  ModuleStepFunction:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub '${ResourcePrefix}${CFDataName}-StateMachine'
      StateMachineType: STANDARD
      RoleArn: !Ref StepFunctionExecutionRoleARN
      DefinitionString: !Ref StepFunctionTemplate
      DefinitionSubstitutions:
        AccountCollectorLambdaARN: !Ref AccountCollectorLambdaARN
        ModuleLambdaARN: !GetAtt LambdaFunction.Arn
        Crawlers: !Sub '["${ResourcePrefix}${CFDataName}-Crawler"]'
        CollectionType: "Payers"
        Params: ''
        Module: !Ref CFDataName
        DeployRegion: !Ref AWS::Region
        Account: !Ref AWS::AccountId
        Prefix: !Ref ResourcePrefix

  ModuleRefreshSchedule:
    Type: 'AWS::Scheduler::Schedule'
    Properties:
      Description: !Sub 'Scheduler for the ODC ${CFDataName} module'
      Name: !Sub '${ResourcePrefix}${CFDataName}-RefreshSchedule'
      ScheduleExpression: !Ref Schedule
      State: ENABLED
      FlexibleTimeWindow:
        Mode: 'OFF'
      Target:
        Arn: !GetAtt ModuleStepFunction.Arn
        RoleArn: !Ref SchedulerExecutionRoleARN

  LambdaAnalyticsExecutor:
    Type: Custom::LambdaAnalyticsExecutor
    Properties:
      ServiceToken: !Ref LambdaAnalyticsARN
      Name: !Ref CFDataName
