AWSTemplateFormatVersion: "2010-09-09"
Description: Retrieves AWS Backup details accross AWS organization
Parameters:
  DatabaseName:
    Type: String
    Description: Name of the Athena database to be created to hold lambda information
    Default: optimization_data
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket that exists or needs to be created to hold backup information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  DestinationBucketARN:
    Type: String
    Description: ARN of the S3 Bucket that exists or needs to be created to hold backup information
  ManagementRoleName:
    Type: String
    Description: The name of the IAM role that will be deployed in the management account which can retrieve AWS Organization data. KEEP THE SAME AS WHAT IS DEPLOYED INTO MANAGEMENT ACCOUNT
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: cost-anomaly
  GlueRoleARN:
    Type: String
    Description: Arn for the Glue Crawler role
  Schedule:
    Type: String
    Description: EventBridge Schedule to trigger the data collection
    Default: "rate(14 days)"
  ResourcePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  LambdaAnalyticsARN:
    Type: String
    Description: Arn of lambda for Analytics
  AccountCollectorLambdaARN:
    Type: String
    Description: Arn of the Account Collector Lambda
  StepFunctionTemplate:
    Type: String
    Description: JSON representation of common StepFunction template
  StepFunctionExecutionRoleARN:
    Type: String
    Description: Common role for Step Function execution
  SchedulerExecutionRoleARN:
    Type: String
    Description: Common role for module Scheduler execution

Outputs:
  StepFunctionARN:
    Description: ARN for the module's Step Function
    Value: !GetAtt ModuleStepFunction.Arn

Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${ResourcePrefix}${CFDataName}-LambdaRole"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Path: /
      Policies:
        - PolicyName: !Sub "${CFDataName}-ManagementAccount-LambdaRole"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: !Sub "arn:aws:iam::*:role/${ManagementRoleName}" # Need to assume a Read role in all Management accounts
        - PolicyName: "S3-Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:PutObject"
                  - "s3:GetObject"
                  - "s3:PutObjectAcl"
                Resource:
                  - !Sub "${DestinationBucketARN}/*"
              - Effect: "Allow"
                Action:
                  - "s3:ListBucket"
                Resource:
                  - !Sub "${DestinationBucketARN}"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ResourcePrefix}${CFDataName}-Lambda-Backup-Records'
      Description: !Sub "Lambda function to retrieve ${CFDataName}"
      Runtime: python3.10
      Architectures: [x86_64]
      Code:
        ZipFile: |
        import os
        import json
        import logging
        from datetime import date, timedelta, datetime
        import boto3
        from boto3.s3.transfer import S3Transfer

        BUCKET = os.environ['BUCKET_NAME']
        ROLE = os.environ['ROLENAME']
        PREFIX = os.environ['PREFIX']
        CRAWLER_NAME = os.environ['CRAWLER_NAME']
        MANAGEMENT_ACCOUNT_IDS = os.environ['MANAGEMENT_ACCOUNT_IDS']

        def start_crawler():
            glue = boto3.client('glue')
            try:
                glue.start_crawler(Name=CRAWLER_NAME)
            except Exception as exc:
                logging.warning(exc)

        def store_data_to_s3(flattened_data, path):
            today = date.today()
            year = today.year
            month = today.strftime('%m')
            day = today.day
            local_file = '/tmp/tmp.json'
            with open(local_file, 'w') as f:
                f.write('\n'.join([json.dumps(result) for result in flattened_data]))
            if os.path.getsize(local_file) == 0:
                print(f"No data in file for {path}")
                return
            s3client = boto3.client('s3')
            key = today.strftime(f"{path}/year={year}/month={month}/day={day}/{year}-{month}-{day}.json")
            print(f"Uploading file {local_file} to {BUCKET}/{key}")
            S3Transfer(s3client).upload_file(local_file, BUCKET, key, extra_args={'ACL': 'bucket-owner-full-control'})
            print('file upload successful')


        def list_backup_jobs(backup, start_date, end_date):
            results = []
            next_token = None
            while True:
                params = dict(
                    ByCompleteAfter= str(start_date),
                    ByCompleteBefore= str(end_date),
                    ByAccountId= '*',
                    MaxResults=100
                )
                if next_token:
                    params['NextToken'] = next_token
                response = backup.list_backup_jobs(**params)
                results += response['BackupJobs']
                if 'NextToken' in response:
                    next_token = response['NextToken']
                else:
                    break
            return results

        def flatten_results(results, parent_key='',sep='_'):
            flattened_results = {}

            for key,value in results.items():
                new_key = parent_key + sep + key if parent_key else key
                if isinstance(value, dict):
                    flattened_results.update(flatten_results(value, new_key, sep=sep))
                elif isinstance(value, datetime):
                    flattened_results[new_key] = value.isoformat()
                else:
                    flattened_results[new_key] = value
            return flattened_results

        def flatten_response(data):
            if isinstance(data, list):
                # Flatten each item in the list
                flattened_data = [flatten_results(item) for item in data]
            else:
                # Flatten the single JSON object
                flattened_data = flatten_results(data)
            return flattened_data

        def calculate_dates(s3_path):
            end_date = datetime.now().date()
            #Returns a list of existing backup jobs for an authenticated account for the last 30 days
            start_date = datetime.now().date() - timedelta(days=30)

            # Check the create time of objects in the S3 bucket
            paginator = boto3.client('s3').get_paginator('list_objects_v2')
            contents = sum( [page.get('Contents', []) for page in paginator.paginate(Bucket=BUCKET, Prefix=s3_path)], [])
            last_modified_date = get_last_modified_date(contents)
            if last_modified_date and last_modified_date >= start_date:
                start_date = last_modified_date
            return start_date, end_date

        def get_last_modified_date(contents):
            last_modified_dates = [obj['LastModified'].date() for obj in contents]
            last_modified_dates_within_30_days = [date for date in last_modified_dates if date >= datetime.now().date() - timedelta(days=30)]
            if last_modified_dates_within_30_days:
                return max(last_modified_dates_within_30_days)
            return None

        def lambda_handler(event, context):
            logger = logging.getLogger()
            sts = boto3.client('sts')
            start_date, end_date = calculate_dates(s3_path=f'{PREFIX}/backup-jobs-data/')
            print(f'start_date={start_date}, end_date={end_date}')
            total_count = 0
            for payer_id in [r.strip() for r in MANAGEMENT_ACCOUNT_IDS.split(',')]:
                ROLE_ARN = f"arn:aws:iam::{payer_id}:role/{ROLE}"
                try:
                    creds = sts.assume_role(RoleArn=ROLE_ARN, RoleSessionName="cross_acct_lambda")['Credentials']
                    assumed_role_session = boto3.session.Session(
                        aws_access_key_id=creds['AccessKeyId'],
                        aws_secret_access_key=creds['SecretAccessKey'],
                        aws_session_token=creds['SessionToken']
                    )
                    backup = assumed_role_session.client("backup", "us-east-1")
                    data = list_backup_jobs(backup, start_date, end_date)
                    print (start_date)
                    print (end_date)
                    print (data)
                    flattened_data = flatten_response(data)
                    print (flattened_data)
                    total_count += len(flattened_data)
                    store_data_to_s3(flattened_data, f'{PREFIX}/backup-jobs-data/payer_id={payer_id}')
                except Exception as exc:
                    print(exc)
            if total_count:
                start_crawler()
            return "Successful"

      Handler: "index.lambda_handler"
      MemorySize: 2688
      Timeout: 600
      Role: !GetAtt LambdaRole.Arn
      Environment:
        Variables:
          BUCKET_NAME: !Ref DestinationBucket
          PREFIX: !Ref CFDataName
          ROLENAME: !Ref ManagementRoleName
          CRAWLER_NAME: !Ref Crawler
          MANAGEMENT_ACCOUNT_IDS: !Ref AWS::AccountId
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"

  LambdaFunctionCopy:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ResourcePrefix}${CFDataName}-Lambda-Copy-Records'
      Description: !Sub "Lambda function to retrieve ${CFDataName}"
      Runtime: python3.10
      Architectures: [x86_64]
      Code:
        ZipFile: |
        import os
        import json
        import logging
        from datetime import date, timedelta, datetime

        import boto3
        from boto3.s3.transfer import S3Transfer

        BUCKET = os.environ['BUCKET_NAME']
        ROLE = os.environ['ROLENAME']
        PREFIX = os.environ['PREFIX']
        CRAWLER_NAME = os.environ['CRAWLER_NAME']
        MANAGEMENT_ACCOUNT_IDS = os.environ['MANAGEMENT_ACCOUNT_IDS']

        def start_crawler():
            glue = boto3.client('glue')
            try:
                glue.start_crawler(Name=CRAWLER_NAME)
            except Exception as exc:
                logging.warning(exc)

        def store_data_to_s3(flattened_data, path):
            today = date.today()
            year = today.year
            month = today.strftime('%m')
            day = today.day
            local_file = '/tmp/tmp.json'
            with open(local_file, 'w') as f:
                f.write('\n'.join([json.dumps(result) for result in flattened_data]))
            if os.path.getsize(local_file) == 0:
                print(f"No data in file for {path}")
                return
            s3client = boto3.client('s3')
            key = today.strftime(f"{path}/year={year}/month={month}/day={day}/{year}-{month}-{day}.json")
            print(f"Uploading file {local_file} to {BUCKET}/{key}")
            S3Transfer(s3client).upload_file(local_file, BUCKET, key, extra_args={'ACL': 'bucket-owner-full-control'})
            print('file upload successful')


        def list_copy_jobs(copy, start_date, end_date):
            results = []
            next_token = None
            while True:
                params = dict(
                    ByCompleteAfter= str(start_date),
                    ByCompleteBefore= str(end_date),
                    ByAccountId= '*',
                    MaxResults=100
                )
                if next_token:
                    params['NextToken'] = next_token
                response = copy.list_copy_jobs(**params)
                results += response['CopyJobs']
                if 'NextToken' in response:
                    next_token = response['NextToken']
                else:
                    break
            return results

        def flatten_results(results, parent_key='',sep='_'):
            flattened_results = {}

            for key,value in results.items():
                new_key = parent_key + sep + key if parent_key else key
                if isinstance(value, dict):
                    flattened_results.update(flatten_results(value, new_key, sep=sep))
                elif isinstance(value, datetime):
                    flattened_results[new_key] = value.isoformat()
                else:
                    flattened_results[new_key] = value
            return flattened_results

        def flatten_response(data):
            if isinstance(data, list):
                # Flatten each item in the list
                flattened_data = [flatten_results(item) for item in data]
            else:
                # Flatten the single JSON object
                flattened_data = flatten_results(data)
            return flattened_data

        def calculate_dates(s3_path):
            end_date = datetime.now().date()
            #Returns a list of existing copy jobs for an authenticated account for the last 30 days
            start_date = datetime.now().date() - timedelta(days=30)

            # Check the create time of objects in the S3 bucket
            paginator = boto3.client('s3').get_paginator('list_objects_v2')
            contents = sum( [page.get('Contents', []) for page in paginator.paginate(Bucket=BUCKET, Prefix=s3_path)], [])
            last_modified_date = get_last_modified_date(contents)
            if last_modified_date and last_modified_date >= start_date:
                start_date = last_modified_date
            return start_date, end_date

        def get_last_modified_date(contents):
            last_modified_dates = [obj['LastModified'].date() for obj in contents]
            last_modified_dates_within_30_days = [date for date in last_modified_dates if date >= datetime.now().date() - timedelta(days=30)]
            if last_modified_dates_within_30_days:
                return max(last_modified_dates_within_30_days)
            return None

        def lambda_handler(event, context):
            logger = logging.getLogger()
            sts = boto3.client('sts')
            start_date, end_date = calculate_dates(s3_path=f'{PREFIX}/copy-jobs-data/')
            print(f'start_date={start_date}, end_date={end_date}')
            total_count = 0
            for payer_id in [r.strip() for r in MANAGEMENT_ACCOUNT_IDS.split(',')]:
                ROLE_ARN = f"arn:aws:iam::{payer_id}:role/{ROLE}"
                try:
                    creds = sts.assume_role(RoleArn=ROLE_ARN, RoleSessionName="cross_acct_lambda")['Credentials']
                    assumed_role_session = boto3.session.Session(
                        aws_access_key_id=creds['AccessKeyId'],
                        aws_secret_access_key=creds['SecretAccessKey'],
                        aws_session_token=creds['SessionToken']
                    )
                    copy = assumed_role_session.client("backup", "us-east-1")
                    data = list_copy_jobs(copy, start_date, end_date)
                    print (start_date)
                    print (end_date)
                    print (data)
                    flattened_data = flatten_response(data)
                    print (flattened_data)
                    total_count += len(flattened_data)
                    store_data_to_s3(flattened_data, f'{PREFIX}/copy-jobs-data/payer_id={payer_id}')
                except Exception as exc:
                    print(exc)
            if total_count:
                start_crawler()
            return "Successful"

      Handler: "index.lambda_handler"
      MemorySize: 2688
      Timeout: 600
      Role: !GetAtt LambdaRole.Arn
      Environment:
        Variables:
          BUCKET_NAME: !Ref DestinationBucket
          PREFIX: !Ref CFDataName
          ROLENAME: !Ref ManagementRoleName
          CRAWLER_NAME: !Ref Crawler
          MANAGEMENT_ACCOUNT_IDS: !Ref AWS::AccountId
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"

  LambdaFunctionRestore:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ResourcePrefix}${CFDataName}-Lambda-Restore-Records'
      Description: !Sub "Lambda function to retrieve ${CFDataName}"
      Runtime: python3.10
      Architectures: [x86_64]
      Code:
        ZipFile: |
        import os
        import json
        import logging
        from datetime import date, timedelta, datetime

        import boto3
        from boto3.s3.transfer import S3Transfer

        BUCKET = os.environ['BUCKET_NAME']
        ROLE = os.environ['ROLENAME']
        PREFIX = os.environ['PREFIX']
        CRAWLER_NAME = os.environ['CRAWLER_NAME']
        MANAGEMENT_ACCOUNT_IDS = os.environ['MANAGEMENT_ACCOUNT_IDS']

        def start_crawler():
            glue = boto3.client('glue')
            try:
                glue.start_crawler(Name=CRAWLER_NAME)
            except Exception as exc:
                logging.warning(exc)

        def store_data_to_s3(flattened_data, path):
            today = date.today()
            year = today.year
            month = today.strftime('%m')
            day = today.day
            local_file = '/tmp/tmp.json'
            with open(local_file, 'w') as f:
                f.write('\n'.join([json.dumps(result) for result in flattened_data]))
            if os.path.getsize(local_file) == 0:
                print(f"No data in file for {path}")
                return
            s3client = boto3.client('s3')
            key = today.strftime(f"{path}/year={year}/month={month}/day={day}/{year}-{month}-{day}.json")
            print(f"Uploading file {local_file} to {BUCKET}/{key}")
            S3Transfer(s3client).upload_file(local_file, BUCKET, key, extra_args={'ACL': 'bucket-owner-full-control'})
            print('file upload successful')


        def list_restore_jobs(restore, start_date, end_date):
            results = []
            next_token = None
            while True:
                params = dict(
                    ByCompleteAfter= str(start_date),
                    ByCompleteBefore= str(end_date),
                    ByAccountId= '*',
                    MaxResults=100
                )
                if next_token:
                    params['NextToken'] = next_token
                response = restore.list_restore_jobs(**params)
                results += response['RestoreJobs']
                if 'NextToken' in response:
                    next_token = response['NextToken']
                else:
                    break
            return results

        def flatten_results(results, parent_key='',sep='_'):
            flattened_results = {}

            for key,value in results.items():
                new_key = parent_key + sep + key if parent_key else key
                if isinstance(value, dict):
                    flattened_results.update(flatten_results(value, new_key, sep=sep))
                elif isinstance(value, datetime):
                    flattened_results[new_key] = value.isoformat()
                else:
                    flattened_results[new_key] = value
            return flattened_results

        def flatten_response(data):
            if isinstance(data, list):
                # Flatten each item in the list
                flattened_data = [flatten_results(item) for item in data]
            else:
                # Flatten the single JSON object
                flattened_data = flatten_results(data)
            return flattened_data

        def calculate_dates(s3_path):
            end_date = datetime.now().date()
            #Returns a list of existing restore jobs for an authenticated account for the last 30 days
            start_date = datetime.now().date() - timedelta(days=30)

            # Check the create time of objects in the S3 bucket
            paginator = boto3.client('s3').get_paginator('list_objects_v2')
            contents = sum( [page.get('Contents', []) for page in paginator.paginate(Bucket=BUCKET, Prefix=s3_path)], [])
            last_modified_date = get_last_modified_date(contents)
            if last_modified_date and last_modified_date >= start_date:
                start_date = last_modified_date
            return start_date, end_date

        def get_last_modified_date(contents):
            last_modified_dates = [obj['LastModified'].date() for obj in contents]
            last_modified_dates_within_30_days = [date for date in last_modified_dates if date >= datetime.now().date() - timedelta(days=30)]
            if last_modified_dates_within_30_days:
                return max(last_modified_dates_within_30_days)
            return None

        def lambda_handler(event, context):
            logger = logging.getLogger()
            sts = boto3.client('sts')
            start_date, end_date = calculate_dates(s3_path=f'{PREFIX}/restore-jobs-data/')
            print(f'start_date={start_date}, end_date={end_date}')
            total_count = 0
            for payer_id in [r.strip() for r in MANAGEMENT_ACCOUNT_IDS.split(',')]:
                ROLE_ARN = f"arn:aws:iam::{payer_id}:role/{ROLE}"
                try:
                    creds = sts.assume_role(RoleArn=ROLE_ARN, RoleSessionName="cross_acct_lambda")['Credentials']
                    assumed_role_session = boto3.session.Session(
                        aws_access_key_id=creds['AccessKeyId'],
                        aws_secret_access_key=creds['SecretAccessKey'],
                        aws_session_token=creds['SessionToken']
                    )
                    restore = assumed_role_session.client("backup", "us-east-1")
                    data = list_restore_jobs(restore, start_date, end_date)
                    print (start_date)
                    print (end_date)
                    print (data)
                    flattened_data = flatten_response(data)
                    print (flattened_data)
                    total_count += len(flattened_data)
                    store_data_to_s3(flattened_data, f'{PREFIX}/restore-jobs-data/payer_id={payer_id}')
                except Exception as exc:
                    print(exc)
            if total_count:
                start_crawler()
            return "Successful"

      Handler: "index.lambda_handler"
      MemorySize: 2688
      Timeout: 600
      Role: !GetAtt LambdaRole.Arn
      Environment:
        Variables:
          BUCKET_NAME: !Ref DestinationBucket
          PREFIX: !Ref CFDataName
          ROLENAME: !Ref ManagementRoleName
          CRAWLER_NAME: !Ref Crawler
          MANAGEMENT_ACCOUNT_IDS: !Ref AWS::AccountId
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"

  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Sub '${ResourcePrefix}${CFDataName}-Crawler'
      Role: !Ref GlueRoleARN
      DatabaseName: !Ref DatabaseName
      Targets:
        S3Targets:
          - Path: !Sub "s3://${DestinationBucket}/${CFDataName}/${CFDataName}-data/"

  ModuleStepFunction:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub '${ResourcePrefix}${CFDataName}-StateMachine'
      StateMachineType: STANDARD
      RoleArn: !Ref StepFunctionExecutionRoleARN
      DefinitionString: !Ref StepFunctionTemplate
      DefinitionSubstitutions:
        AccountCollectorLambdaARN: !Ref AccountCollectorLambdaARN
        ModuleLambdaARN: !GetAtt LambdaFunction.Arn
        Crawlers: !Sub '["${ResourcePrefix}${CFDataName}-Crawler"]'
        CollectionType: "Payers"
        Params: ''
        Module: !Ref CFDataName
        DeployRegion: !Ref AWS::Region
        Account: !Ref AWS::AccountId
        Prefix: !Ref ResourcePrefix

  ModuleRefreshSchedule:
    Type: 'AWS::Scheduler::Schedule'
    Properties:
      Description: !Sub 'Scheduler for the ODC ${CFDataName} module'
      Name: !Sub '${ResourcePrefix}${CFDataName}-RefreshSchedule'
      ScheduleExpression: !Ref Schedule
      State: ENABLED
      FlexibleTimeWindow:
        Mode: 'OFF'
      Target:
          Arn: !GetAtt ModuleStepFunction.Arn
          RoleArn: !Ref SchedulerExecutionRoleARN


  FinalizeSetup:
    DependsOn:
      - LambdaFunction
      - LambdaFunctionCopy
      - LambdaFunctionRestore
    Type: Custom::CustomResource
    Properties:
      ServiceToken: !GetAtt LambdaFunctionRestore.Arn

  LambdaAnalyticsExecutor:
    Type: Custom::LambdaAnalyticsExecutor
    Properties:
      ServiceToken: !Ref LambdaAnalyticsARN
      Name: !Ref CFDataName
