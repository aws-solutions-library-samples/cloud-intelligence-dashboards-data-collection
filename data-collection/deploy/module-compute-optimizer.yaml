AWSTemplateFormatVersion: '2010-09-09'
Description: Retrieves AWS Compute Optimizer information from across an organization
Parameters:
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket to be created to hold data information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  ManagementRoleName:
    Type: String
    Description: The name of the IAM role that will be deployed in the management account which can retrieve AWS Organization data. KEEP THE SAME AS WHAT IS DEPLOYED INTO MANAGEMENT ACCOUNT
  ManagementAccountID:
    Type: String
    AllowedPattern: ([a-z0-9\-, ]*?$)
    Description: "(Ex: 123456789,098654321,789054312) List of Payer IDs you wish to collect data for. Can just be one Accounts"
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: compute-optimizer
  Schedule:
    Type: String
    Description: EventBridge Schedule to trigger the data collection
    Default: "rate(14 days)"
  ResourcePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  BucketPrefix:
    Type: String
    Default: 'costoptimization'
    Description: This prefix will be used for buckets creation
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9\-])$)
  RegionsInScope:
    Type: String
    Description: "Comma Delimited list of AWS regions from which data about resources will be collected. Example: us-east-1,eu-west-1,ap-northeast-1"
  LambdaAnalyticsARN:
    Type: String
    Description: Arn of lambda for Analytics
  IncludeMemberAccounts:
    Type: String
    AllowedValues: ["yes", "no"]
    Default: 'yes'
    Description: Include Member Accounts or not. Set to 'yes' when working with payer 
  AccountCollectorLambdaARN:
    Type: String
    Description: Arn of the Account Collector Lambda
  StepFunctionTemplate:
    Type: String
    Description: JSON representation of common StepFunction template
  StepFunctionExecutionRoleARN:
    Type: String
    Description: Common role for Step Function execution
  SchedulerExecutionRoleARN:
    Type: String
    Description: Common role for module Scheduler execution

Outputs:
  StepFunctionARN:
    Description: ARN for the module's Step Function
    Value: !GetAtt ModuleStepFunction.Arn

Resources:
  StackSetAdminRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName:  StackSetAdminRole
      Description: "This role is used by cloudformation to create StackSets in different regions. See https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs.html"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: cloudformation.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Policies:
        - PolicyName: StackSetAdminRolePolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: sts:AssumeRole
                Resource: !Sub "arn:aws:iam::*:role/${ResourcePrefix}ComputeOptimizer-StackSetExecutionRole"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  StackSetExecutionRole:  #see https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs.html
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${ResourcePrefix}ComputeOptimizer-StackSetExecutionRole"
      Description: "This role is used by cloudformation to create StackSets in different regions. See https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-prereqs.html"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              AWS: !GetAtt StackSetAdminRole.Arn
            Action: sts:AssumeRole
      Policies:
        - PolicyName: StackSetParameterExecutionPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - cloudformation:CreateStack
                  - cloudformation:DescribeStacks
                  - cloudformation:DeleteStack
                  - cloudformation:UpdateStack
                Resource: !Sub 'arn:aws:cloudformation:*:${AWS::AccountId}:stack/StackSet-${BucketPrefix}${AWS::AccountId}-comp-optim-buckets*'
        - PolicyName: ResourcesPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:CreateBucket
                  - s3:DeleteBucket
                  - s3:ListBucket
                  - s3:Put*
                  - s3:Set*
                  - s3:Get*
                  - s3:Replicate*
                Resource:
                  - !Sub 'arn:aws:s3:::${BucketPrefix}${AWS::AccountId}-*'
              - Effect: Allow
                Action:
                  - iam:GetRole
                  - iam:CreateRole
                  - iam:DeleteRole
                  - iam:PassRole
                  - iam:DeleteRolePolicy
                  - iam:GetRolePolicy
                  - iam:PutRolePolicy
                Resource:
                  - !Sub 'arn:aws:iam::${AWS::AccountId}:role/StackSet-${BucketPrefix}*' # Shorter version of StackSetName
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  ReplicationBucketsStackSet:
    Type: AWS::CloudFormation::StackSet
    # DependsOn: # managed via tags. DependsOn does not work in this case and Role can be deleted before StackSet
    #  - StackSetExecutionRole
    Properties:
      Description: !Sub "S3 buckets in multiple regions replicating data to s3://${DestinationBucket}."
      PermissionModel: SELF_MANAGED
      ManagedExecution:
        Active: true
      Parameters:
        - ParameterKey: Name
          ParameterValue: !Sub "${BucketPrefix}${AWS::AccountId}"
        - ParameterKey: DestinationBucket
          ParameterValue: !Ref DestinationBucket
        - ParameterKey: ManagementAccountID
          ParameterValue: !Ref ManagementAccountID
      StackInstancesGroup:
        - DeploymentTargets:
            Accounts:
              - !Ref "AWS::AccountId"
          Regions: !Split [ ',', !Ref RegionsInScope]
      Capabilities:
        - CAPABILITY_IAM
        - CAPABILITY_NAMED_IAM
      StackSetName: !Sub "${BucketPrefix}${AWS::AccountId}-comp-optim-buckets"
      AdministrationRoleARN: !GetAtt StackSetAdminRole.Arn
      ExecutionRoleName: !Sub "${ResourcePrefix}ComputeOptimizer-StackSetExecutionRole"
      TemplateBody: |
        AWSTemplateFormatVersion: '2010-09-09'
        Description: This template creates a bucket in a given region and configure a replication to the main bucket
        Parameters:
          Name:
            Type: String
            Description: Prefix of regional bucket name
          ManagementAccountID:
            Type: String
            Description: Comma separated IDs of Management Accounts that will send data to the bucket
          DestinationBucket:
            Type: String
            Description: A name of the main bucket where all data will be stored
        Resources:
          BucketPolicy:
            Type: AWS::S3::BucketPolicy
            Properties:
              Bucket: !Ref S3Bucket
              PolicyDocument:
                Version: 2012-10-17
                Statement:
                - Effect: Allow
                  Principal:
                    Service: compute-optimizer.amazonaws.com
                  Action:
                  - s3:GetBucketAcl
                  - s3:GetBucketPolicyStatus
                  Resource: !Sub "arn:aws:s3:::${Name}-${AWS::Region}"
                - Effect: Allow
                  Principal:
                    Service: compute-optimizer.amazonaws.com
                  Action: s3:PutObject
                  Condition:
                    StringEquals:
                          s3:x-amz-acl:  bucket-owner-full-control
                          aws:SourceAccount: !Split [',', !Ref ManagementAccountID]
                  Resource: !Sub "arn:aws:s3:::${Name}-${AWS::Region}/*"
                - Effect: Deny
                  Principal: "*"
                  Action: s3:*
                  Condition:
                    Bool:
                      aws:SecureTransport: 'false'
                  Resource:
                    - !Sub "arn:aws:s3:::${Name}-${AWS::Region}"
                    - !Sub "arn:aws:s3:::${Name}-${AWS::Region}/*"
          ReplicaRole:
            Type: AWS::IAM::Role
            Properties:
              AssumeRolePolicyDocument:
                Statement:
                - Action: ['sts:AssumeRole']
                  Effect: Allow
                  Principal:
                    Service: [s3.amazonaws.com]
          ReplicaPolicy:
            Type: AWS::IAM::Policy
            Properties:
              PolicyDocument:
                Statement:
                - Action:
                  - s3:GetReplicationConfiguration
                  - s3:ListBucket
                  Resource: !Sub arn:aws:s3:::${Name}-${AWS::Region}
                  Effect: 'Allow'
                - Action:
                  - s3:GetObjectVersionForReplication
                  - s3:GetObjectVersionAcl
                  Resource: !Sub arn:aws:s3:::${Name}-${AWS::Region}/*
                  Effect: 'Allow'
                - Action:
                  - s3:ReplicateObject
                  - s3:ReplicateTags
                  - s3:GetObjectVersionTagging
                  Effect: 'Allow'
                  Resource: !Sub arn:aws:s3:::${DestinationBucket}/*
              PolicyName: ReplicaPolicy
              Roles: [!Ref 'ReplicaRole']
          S3Bucket:
            Type: AWS::S3::Bucket
            DeletionPolicy: Delete
            Properties:
              BucketName: !Sub ${Name}-${AWS::Region}
              BucketEncryption:
                ServerSideEncryptionConfiguration:
                  - ServerSideEncryptionByDefault:
                      SSEAlgorithm: AES256
              ReplicationConfiguration:
                Role: !GetAtt [ReplicaRole, Arn]
                Rules:
                  - Id: Replication to the main bucket
                    Priority: 2
                    Filter:
                      Prefix: ''
                    Destination:
                      Bucket: !Sub arn:aws:s3:::${DestinationBucket}
                      StorageClass: STANDARD
                    DeleteMarkerReplication:
                      Status: Disabled
                    Status: Enabled
              VersioningConfiguration:
                Status: Enabled
              PublicAccessBlockConfiguration:
                BlockPublicAcls : true
                BlockPublicPolicy : true
                IgnorePublicAcls : true
                RestrictPublicBuckets : true
              LifecycleConfiguration:
                Rules:
                  - Id: Cleanup
                    Prefix: ''
                    Status: Enabled
                    ExpirationInDays: 1
                  - Id: NoncurrentCleanup
                    Prefix: ''
                    Status: Enabled
                    NoncurrentVersionExpiration:
                      NoncurrentDays: 1
                      NewerNoncurrentVersions: 1
      Tags: # Hacky way to manage dependencies
        - Key: IgnoreMeIamOnlyWorkaround
          Value: !GetAtt StackSetExecutionRole.Arn

  LambdaTriggerExportRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${ResourcePrefix}${CFDataName}-LambdaRole"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Path: /
      Policies:
        - PolicyName: !Sub "${CFDataName}-ManagementAccount-LambdaRole"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: !Sub "arn:aws:iam::*:role/${ManagementRoleName}" # Need to assume a Read role in all Management Accounts
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  LambdaTriggerExport:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ResourcePrefix}${CFDataName}-Lambda'
      Description: "LambdaFunction to start ComputeOptimizer export jobs"
      Runtime: python3.10
      Architectures: [x86_64]
      Environment:
        Variables:
          REGIONS: !Ref RegionsInScope
          BUCKET_NAME: !Sub "${BucketPrefix}${AWS::AccountId}"
          INCLUDE_MEMBER_ACCOUNTS: !Ref IncludeMemberAccounts
          ROLE_NAME: !Ref ManagementRoleName
          MANAGEMENT_ACCOUNT_IDS: !Ref ManagementAccountID
      Code:
        ZipFile: |
          import os
          import json
          import logging
          from datetime import date, datetime

          import boto3
          import botocore

          class ParamsBase():
              def __init__(self, env, additional_params):
                  logger.setLevel(getattr(logging, env.get('LOG_LEVEL', 'INFO').upper(), logging.INFO))
                  logger.debug("Loading parameters")
                  self.collection_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                  self.tmp_file = "/tmp/tmp.json"
                  self.bucket = env["BUCKET_NAME"]
                  self.role_name = env["ROLE_NAME"]
                  self.module_name = env["MODULE_NAME"]
                  self.regions = [r.strip() for r in env.get("REGIONS","us-east-1").split(',') if r]
                  self.boto_config = None
                  self.additional_params = additional_params

          """
          Custom implementation:
          These functions are specific to this module.
          """
          class Params(ParamsBase):
              """ Tailor this class to add any unique configuration """
              def __init__(self, env, additional_params):
                  try:
                      super().__init__(env, additional_params)
                      self.include_member_accounts = env.get("INCLUDE_MEMBER_ACCOUNTS", 'yes').lower() == 'yes'
                  except (KeyError, AttributeError) as exc:
                      raise CidCriticalError(f"Invalid parameters supplied", exc)

          def get_api_data(account, params, region):
              """ Method to call the necessary APIs and process the data """
              logger.debug(f"Entering get_api_data for region '{region}'")
              client = get_client_with_role(params.role_name, account.account_id, region=region, service="compute-optimizer", params=params)
              results = []

              export_funcs = {
                  'ec2_instance': client.export_ec2_instance_recommendations,
                  'auto_scale':   client.export_auto_scaling_group_recommendations,
                  'lambda':       client.export_lambda_function_recommendations,
                  'ebs_volume':   client.export_ebs_volume_recommendations,
                  'ecs_service':  client.export_ecs_service_recommendations,
              }
              bkt = f"{params.bucket}-{region}"
              logger.debug(f"Target bucket is {bkt}")
              for name, func in export_funcs.items():
                  try:
                      res = func(
                          includeMemberAccounts=params.include_member_accounts,
                          s3DestinationConfig={'bucket': bkt, 'keyPrefix': get_s3_key(account, params, name)}
                      )
                      logger.info(f"{region} {name} export queued. JobId: {res['jobId']}")
                  except Exception as exc:
                      if 'LimitExceededException' in str(exc):
                          logger.info(f"{region} {name} export is already in progress.")
                      else:
                          CidNonCriticalError(f"(NonCriticalError) in get_api_data with {region} {name}", exc) #no need to raise

              logger.info(f"API calls completed. NOTE: No files will be uploaded to the main bucket by this function. Compute Optimizer manages them.")
              return results

          def get_s3_key(account, params, name): #pylint: disable=unused-argument
              return datetime.now().strftime(
                  f"compute_optimizer/compute_optimizer_{name}/payer_id={account.payer_id}/year=%Y/month=%m")

          """
          Common implementation:
          These functions and classes are structured for common usage
          patterns across different modules
          """
          logger = logging.getLogger()
          for h in logger.handlers:
              h.setFormatter(logging.Formatter("[%(levelname)s] %(message)s (%(aws_request_id)s)"))
          logger = logging.getLogger(__name__)
          logger.setLevel(logging.INFO)

          def lambda_handler(event, context): #pylint: disable=unused-argument
              """ Common structured entry for Lambda invocation. Core processing done in main()."""
              logger.info(f"Incoming event: {json.dumps(event)}")
              status_code = 500
              try:
                  main(json.loads(event["account"]), event.get('params', ''))
                  status_code = 200
              except KeyError as exc:
                  raise CidCriticalError(f"Account is not defined in the incoming event data. Please do not trigger this function manually. Use the corresponding {os.environ['MODULE_NAME']} state machine in Step Functions instead.")
              except CidNonCriticalError as exc:
                  status_code = 200
              except CidCriticalError as exc:
                  raise exc
              except Exception as exc:
                  raise CidCriticalError(f"(UnhandledExceptionError) in this module", exc)
              finally:
                  return {"statusCode": status_code}

          def main(account_json, additional_params=None):
              """ Method to orchestrate the retrieval and storage of API data """
              logger.debug(f"Entering main")
              try:
                  account = Account(account_json)
                  logger.info(f"Entering main for account: {account.account_id}")
                  params = Params(os.environ, additional_params)
                  for region in params.regions:
                      logger.info(f"Processing for region '{region}'")
                      records = get_api_data(account, params, region)
                      if len(records) > 0:
                          store_to_temp(records, params)
                          upload_to_s3(account, params, region)
                      else:
                          logger.info(f"No file uploaded for region '{region}'")
                  logger.info(f"Exiting main without error")
              except CidError as exc:
                  raise exc
              except (botocore.exceptions.ClientError, ClientAccessError) as exc:
                  raise CidCriticalError(f"Possible role misconfiguration for {params.role_name}", exc)
              except Exception as exc:
                  raise CidCriticalError(f"(UnhandledExceptionError)", exc)

          def store_to_temp(records, params):
              """ Takes the list of processed records and moves them to a temp file """
              logger.debug("Entering store_to_temp")
              count = 0
              try:
                  with open(params.tmp_file, "w", encoding='utf-8') as f:
                      for record in records:
                          f.write(json.dumps(
                              record,
                              default=lambda x: x.isoformat() if isinstance(x, (date, datetime)) else None)
                              + "\n"
                          )
                          count += 1

              except Exception as exc:
                  raise CidCriticalError(f"Unhandled exception in store_to_temp", exc)

              logger.info(f"Stored {count} record(s) in temp file")
              return count

          def upload_to_s3(account, params, region=None):
              """ Moves the processed API data from the temp file to the designated S3 bucket """
              logger.debug(f"Entering upload_to_s3 for bucket {params.bucket}")
              try:
                  key = get_s3_key(account, params, region)
                  boto3.client('s3').upload_file(params.tmp_file, params.bucket, key)
                  logger.info(f"Data stored to s3://{params.bucket}/{key}")
                  return True

              except Exception as exc:
                  raise CidCriticalError("Exception in upload_to_s3", exc)

          def get_client_with_role(role_name, account_id, service, region, params):
              """ Assumes the designated data gathering read-only role and instantiates a boto3 client with it """
              logger.debug(f"Entering get_client_with_role to get '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'")
              try:
                  credentials = boto3.client('sts').assume_role(
                      RoleArn=f"arn:aws:iam::{account_id}:role/{role_name}",
                      RoleSessionName="data_collection"
                  )['Credentials']
                  logger.debug("Successfully assumed role, now getting client")
                  client = boto3.client(
                      service,
                      region_name = region,
                      aws_access_key_id = credentials['AccessKeyId'],
                      aws_secret_access_key = credentials['SecretAccessKey'],
                      aws_session_token = credentials['SessionToken'],
                      config = params.boto_config
                  )
                  logger.info(f"Successfully created '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'")
                  return client

              except Exception as exc:
                  raise ClientAccessError(exc, role_name, account_id, service, region)

          # Helper classes
          class Account():
              def __init__(self, account_json: dict):
                  try:
                      self.account_id = account_json["account_id"]
                      self.account_name = account_json["account_name"]
                      self.payer_id = account_json["payer_id"]
                  except KeyError:
                      raise CidCriticalError(f"Invalid account data passed {account_json}")

          class CidError(Exception):
              def __init__(self, message="", exc=None):
                  try:
                      message = f"({type(exc).__name__}) exception. {message}" if exc else message
                      if type(self) == CidNonCriticalError.__class__:
                          logger.warning(message)
                      else:
                          logger.error(message)
                      super().__init__(message)
                  except Exception as exc:
                      pass
          class CidNonCriticalError(CidError):
              def __init__(self, message="", exc=None):
                  super().__init__(message, exc)
          class CidCriticalError(CidError):
              def __init__(self, message="", exc=None):
                  super().__init__(message, exc)
          class ClientAccessError(Exception):
              def __init__(self, exc, role_name, account_id, service, region):
                  message = f"({type(exc).__name__}) exception: '{exc}' when getting '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'"
                  logger.warning(message)
                  super().__init__(message)
      Handler: index.lambda_handler
      MemorySize: 2688
      Timeout: 300
      Role: !GetAtt LambdaTriggerExportRole.Arn
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"

  ModuleStepFunction:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub '${ResourcePrefix}${CFDataName}-StateMachine'
      StateMachineType: STANDARD
      RoleArn: !Ref StepFunctionExecutionRoleARN
      DefinitionString: !Ref StepFunctionTemplate
      DefinitionSubstitutions:
        AccountCollectorLambdaARN: !Ref AccountCollectorLambdaARN
        ModuleLambdaARN: !GetAtt LambdaTriggerExport.Arn
        Crawlers: '[]'
        CollectionType: "Payers"
        Params: ''
        Module: !Ref CFDataName
        DeployRegion: !Ref AWS::Region
        Account: !Ref AWS::AccountId
        Prefix: !Ref ResourcePrefix

  ModuleRefreshSchedule:
    Type: 'AWS::Scheduler::Schedule'
    Properties:
      Description: !Sub 'Scheduler for the ODC ${CFDataName} module'
      Name: !Sub '${ResourcePrefix}${CFDataName}-RefreshSchedule'
      ScheduleExpression: !Ref Schedule
      State: ENABLED
      FlexibleTimeWindow:
        Mode: 'OFF'
      Target:
          Arn: !GetAtt ModuleStepFunction.Arn
          RoleArn: !Ref SchedulerExecutionRoleARN

  CustomResourceFunctionFinalize:
    Type: AWS::Lambda::Function
    Properties:
      Role: !GetAtt 'FinalizeLambdaExecutionRole.Arn'
      FunctionName: !Sub '${ResourcePrefix}${CFDataName}-Lambda-FinalizeSetup-DoNotRun'
      Description: "Lambda to Finalize Setup (call trigger lambda) or Start teardown (Cleanup buckets)"
      Runtime: python3.10
      Architectures: [ arm64 ]
      Code:
        ZipFile: |
          import os
          import uuid
          import json
          import boto3
          import botocore
          import urllib3
          import logging

          REGIONS = os.environ['REGIONS'].replace(' ', '').split(',')
          BUCKET_PREFIX = os.environ['BUCKET_PREFIX']
          LAMBDA_ARN = os.environ['TRIGGER_LAMBDA']

          logger = logging.getLogger(__name__)
          logger.setLevel(getattr(logging, os.environ.get('LOG_LEVEL', 'INFO').upper(), logging.INFO))

          def lambda_handler(event, context): #pylint: disable=unused-argument
              logger.info(event)
              type_ = event.get('RequestType', 'Undef')
              if type_ == 'Create': res = on_create()
              elif type_ == 'Delete': res = on_delete()
              else: res = (True, f"Not supported operation: {type_}")
              response_data = {'Reason': res[1], 'uuid': str(uuid.uuid1()) }

              url = event.get('ResponseURL')
              body = {}
              body['Status'] = 'SUCCESS' if res[0] else 'FAILED'
              body['Reason'] = str(res[1]) + '\nSee the details in CloudWatch Log Stream: ' + context.log_stream_name
              body['PhysicalResourceId'] = context.log_stream_name
              body['StackId'] = event.get('StackId')
              body['RequestId'] = event.get('RequestId')
              body['LogicalResourceId'] = event.get('LogicalResourceId')
              body['NoEcho'] = False
              body['Data'] = response_data
              logger.info(body)
              if not url: return
              json_body=json.dumps(body)
              try:
                  http = urllib3.PoolManager()
                  response = http.request('PUT', url, body=json_body, headers={'content-type' : '', 'content-length' : str(len(json_body))}, retries=False)
                  logger.info(f"Status code: {response}" )
              except Exception as exc:
                  logger.info("send(..) failed executing requests.put(..): " + str(exc))

          def on_create():
              try:
                  name = LAMBDA_ARN.split(':')[-1]
                  boto3.client('lambda').invoke(FunctionName=name, InvocationType="RequestResponse", Payload='{}')
                  return (True, f'INFO: Invoked {name}')
              except Exception as exc:
                  return (True, f'ERROR: error invoking lambda {exc}')

          def on_delete():
              # Delete all buckets (CF cannot delete if they are non-empty)
              s3 = boto3.resource('s3')
              log = []
              for region in REGIONS:
                  name = BUCKET_PREFIX + "-" + region
                  try:
                      bucket = s3.Bucket(name)
                      bucket.object_versions.delete()
                      log.append(f'INFO:  {name} is empty now')
                      bucket.delete()
                      log.append(f'INFO:  {name} deleted')
                  except botocore.exceptions.ClientError as exc:
                      status = exc.response["ResponseMetadata"]["HTTPStatusCode"]
                      errcode = exc.response["Error"]["Code"]
                      if status == 404:
                          log.append(f'INFO:  {name} - {errcode}')
                      else:
                          log.append(f'ERROR: {name} - {errcode}')
                  except Exception as exc:
                      log.append(f'ERROR: {name} Error: {exc}')
              logger.info('\n'.join(log))
              return (True, '\n'.join(log))
      Handler: 'index.lambda_handler'
      MemorySize: 2688
      Timeout: 300
      Environment:
        Variables:
          REGIONS: !Ref RegionsInScope
          BUCKET_PREFIX: !Sub "${BucketPrefix}${AWS::AccountId}"
          TRIGGER_LAMBDA:  !GetAtt LambdaTriggerExport.Arn
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"

  FinalizeLambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: /
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - lambda:InvokeAsync
                  - lambda:InvokeFunction
                Resource: !GetAtt LambdaTriggerExport.Arn
              - Effect: Allow
                Action:
                  - s3:DeleteObject
                  - s3:DeleteObjectVersion
                Resource: !Sub 'arn:aws:s3:::${BucketPrefix}${AWS::AccountId}-*/*'
              - Effect: Allow
                Action:
                  - s3:ListBucketVersions
                  - s3:DeleteBucket
                Resource: !Sub 'arn:aws:s3:::${BucketPrefix}${AWS::AccountId}-*'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

  FinalizeSetup:
    DependsOn:
      - LambdaTriggerExport
      - ReplicationBucketsStackSet
      - FinalizeLambdaExecutionRole
    Type: Custom::CustomResource
    Properties:
      ServiceToken: !GetAtt CustomResourceFunctionFinalize.Arn

  LambdaAnalyticsExecutor:
    Type: Custom::LambdaAnalyticsExecutor
    Properties:
      ServiceToken: !Ref LambdaAnalyticsARN
      Name: !Ref CFDataName
