AWSTemplateFormatVersion: "2010-09-09"
Description: Retrieves AWS Cost Explorer Cost Anomalies details accross AWS organization
Parameters:
  DatabaseName:
    Type: String
    Description: Name of the Athena database to be created to hold lambda information
    Default: optimization_data
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket that exists or needs to be created to hold costanomaly information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  DestinationBucketARN:
    Type: String
    Description: ARN of the S3 Bucket that exists or needs to be created to hold costanomaly information
  ManagementRoleName:
    Type: String
    Description: The name of the IAM role that will be deployed in the management account which can retrieve AWS Organization data. KEEP THE SAME AS WHAT IS DEPLOYED INTO MANAGEMENT ACCOUNT
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: cost-anomaly
  GlueRoleARN:
    Type: String
    Description: Arn for the Glue Crawler role
  Schedule:
    Type: String
    Description: EventBridge Schedule to trigger the data collection
    Default: "rate(14 days)"
  ResourcePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  LambdaAnalyticsARN:
    Type: String
    Description: Arn of lambda for Analytics
  AccountCollectorLambdaARN:
    Type: String
    Description: Arn of the Account Collector Lambda
  StepFunctionTemplate:
    Type: String
    Description: JSON representation of common StepFunction template
  StepFunctionExecutionRoleARN:
    Type: String
    Description: Common role for Step Function execution
  SchedulerExecutionRoleARN:
    Type: String
    Description: Common role for module Scheduler execution

Outputs:
  StepFunctionARN:
    Description: ARN for the module's Step Function
    Value: !GetAtt ModuleStepFunction.Arn

Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${ResourcePrefix}${CFDataName}-LambdaRole"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Path: /
      Policies:
        - PolicyName: !Sub "${CFDataName}-ManagementAccount-LambdaRole"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: !Sub "arn:aws:iam::*:role/${ManagementRoleName}" # Need to assume a Read role in all Management accounts
        - PolicyName: "S3-Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:PutObject"
                  - "s3:GetObject"
                  - "s3:PutObjectAcl"
                Resource:
                  - !Sub "${DestinationBucketARN}/*"
              - Effect: "Allow"
                Action:
                  - "s3:ListBucket"
                Resource:
                  - !Sub "${DestinationBucketARN}"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ResourcePrefix}${CFDataName}-Lambda'
      Description: !Sub "Lambda function to retrieve ${CFDataName}"
      Runtime: python3.10
      Architectures: [x86_64]
      Code:
        ZipFile: |
          import os
          import json
          import logging
          from datetime import date, timedelta, datetime
          import boto3
          from boto3.s3.transfer import S3Transfer

          BUCKET = os.environ['BUCKET_NAME']
          ROLE = os.environ['ROLENAME']
          PREFIX = os.environ['PREFIX']

          logger = logging.getLogger(__name__)
          logger.setLevel(getattr(logging, os.environ.get('LOG_LEVEL', 'INFO').upper(), logging.INFO))

          def lambda_handler(event, context):
              logger.info(f"Event data {json.dumps(event)}")
              if 'account' not in event:
                  raise ValueError(
                      "Please do not trigger this Lambda manually."
                      "Find the corresponding state machine in Step Functions and Trigger from there."
                  )
              start_date, end_date = calculate_dates(s3_path=f'{PREFIX}/cost-anomaly-data/')
              logger.info(f'start_date={start_date}, end_date={end_date}')
              total_count = 0
              account = json.loads(event["account"])
              try:
                  creds = assume_role(ROLE, account["account_id"])
                  ce = get_client(creds, "us-east-1", "ce")
                  data = get_ce_costanomaly(ce, start_date, end_date)
                  flattened_data = flatten_results(data)
                  total_count += len(flattened_data)
                  store_data_to_s3(flattened_data, f'{PREFIX}/cost-anomaly-data/payer_id={account["account_id"]}')
              except Exception as exc:
                  logging.warning(exc)

              return "Successful"

          def store_data_to_s3(flattened_data, path):
              today = date.today()
              year = today.year
              month = today.strftime('%m')
              day = today.day
              local_file = '/tmp/tmp.json'
              with open(local_file, 'w') as f:
                  f.write('\n'.join([json.dumps(result) for result in flattened_data]))
              if os.path.getsize(local_file) == 0:
                  print(f"No data in file for {path}")
                  return
              s3client = boto3.client('s3')
              key = today.strftime(f"{path}/year={year}/month={month}/day={day}/{year}-{month}-{day}.json")
              print(f"Uploading file {local_file} to {BUCKET}/{key}")
              S3Transfer(s3client).upload_file(local_file, BUCKET, key, extra_args={'ACL': 'bucket-owner-full-control'})
              print('file upload successful')

          def get_ce_costanomaly(ce, start_date, end_date):
              results = []
              next_token = None
              while True:
                  params = dict(
                      DateInterval={
                        'StartDate': str(start_date),
                        'EndDate': str(end_date)
                      },
                      MaxResults=100,
                  )
                  if next_token:
                      params['NextPageToken'] = next_token
                  response = ce.get_anomalies(**params)
                  results += response['Anomalies']
                  if 'NextPageToken' in response:
                      next_token = response['NextPageToken']
                  else:
                      break
              return results

          def flatten_results(results):
              flattened_results = []
              for anomaly in results:
                  flattened_anomaly = {
                      'AnomalyId': anomaly['AnomalyId'],
                      'AnomalyStartDate': anomaly['AnomalyStartDate'],
                      'AnomalyEndDate': anomaly['AnomalyEndDate'],
                      'DimensionValue': anomaly['DimensionValue'],
                      'MaxImpact': anomaly['Impact']['MaxImpact'],
                      'TotalActualSpend': anomaly['Impact']['TotalActualSpend'],
                      'TotalExpectedSpend': anomaly['Impact']['TotalExpectedSpend'],
                      'TotalImpact': anomaly['Impact']['TotalImpact'],
                      'TotalImpactpercentage': anomaly['Impact'].get('TotalImpactPercentage', 0),
                      'MonitorArn': anomaly['MonitorArn'],
                      'LinkedAccount': anomaly['RootCauses'][0].get('LinkedAccount'),
                      'LinkedAccountName': anomaly['RootCauses'][0].get('LinkedAccountName'),
                      'Region': anomaly['RootCauses'][0].get('Region'),
                      'Service': anomaly['RootCauses'][0].get('Service'),
                      'UsageType': anomaly['RootCauses'][0].get('UsageType')
                  }
                  flattened_results.append(flattened_anomaly)
              return flattened_results

          def calculate_dates(s3_path):
              end_date = datetime.now().date()
              start_date = datetime.now().date() - timedelta(days=90) #Cost anomalies are available for last 90days
              # Check the create time of objects in the S3 bucket
              paginator = boto3.client('s3').get_paginator('list_objects_v2')
              contents = sum( [page.get('Contents', []) for page in paginator.paginate(Bucket=BUCKET, Prefix=s3_path)], [])
              last_modified_date = get_last_modified_date(contents)
              if last_modified_date and last_modified_date >= start_date:
                  start_date = last_modified_date
              return start_date, end_date

          def get_last_modified_date(contents):
              last_modified_dates = [obj['LastModified'].date() for obj in contents]
              last_modified_dates_within_90_days = [date for date in last_modified_dates if date >= datetime.now().date() - timedelta(days=90)]
              if last_modified_dates_within_90_days:
                  return max(last_modified_dates_within_90_days)
              return None

          def assume_role(role_name, payer_id):
              logging.debug("Assuming role ...")
              try:
                  creds = boto3.client('sts').assume_role(
                        RoleArn=f"arn:aws:iam::{payer_id}:role/{role_name}",
                        RoleSessionName="data_collection"
                  )['Credentials']
                  logging.info(f"Successfully assumed role")
                  return creds
              except Exception as exc:
                  logger.error(f"Error '{exc}' assuming role: role_name={role_name}, payer_id={payer_id}")
                  raise RuntimeError("Error building client")

          def get_client(credentials, region, service):
              logging.debug("Getting session from credentials ...")
              try:
                  service_client = boto3.client(
                        service,
                        region_name = region,
                        aws_access_key_id=credentials['AccessKeyId'],
                        aws_secret_access_key=credentials['SecretAccessKey'],
                        aws_session_token=credentials['SessionToken'],
                  )
                  logging.info("Successfully got client")
                  return service_client
              except Exception as exc:
                  logger.error(f"Error '{exc}' building client: service={service}, region={region}")
                  raise RuntimeError("Error building client")
                    
      Handler: "index.lambda_handler"
      MemorySize: 2688
      Timeout: 600
      Role: !GetAtt LambdaRole.Arn
      Environment:
        Variables:
          BUCKET_NAME: !Ref DestinationBucket
          PREFIX: !Ref CFDataName
          ROLENAME: !Ref ManagementRoleName
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"

  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Sub '${ResourcePrefix}${CFDataName}-Crawler'
      Role: !Ref GlueRoleARN
      DatabaseName: !Ref DatabaseName
      Targets:
        S3Targets:
          - Path: !Sub "s3://${DestinationBucket}/${CFDataName}/${CFDataName}-data/"

  ModuleStepFunction:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub '${ResourcePrefix}${CFDataName}-StateMachine'
      StateMachineType: STANDARD
      RoleArn: !Ref StepFunctionExecutionRoleARN
      DefinitionString: !Ref StepFunctionTemplate
      DefinitionSubstitutions:
        AccountCollectorLambdaARN: !Ref AccountCollectorLambdaARN
        ModuleLambdaARN: !GetAtt LambdaFunction.Arn
        Crawlers: !Sub '["${ResourcePrefix}${CFDataName}-Crawler"]'
        CollectionType: "Payers"
        Params: ''
        Module: !Ref CFDataName
        DeployRegion: !Ref AWS::Region
        Account: !Ref AWS::AccountId
        Prefix: !Ref ResourcePrefix

  ModuleRefreshSchedule:
    Type: 'AWS::Scheduler::Schedule'
    Properties:
      Description: !Sub 'Scheduler for the ODC ${CFDataName} module'
      Name: !Sub '${ResourcePrefix}${CFDataName}-RefreshSchedule'
      ScheduleExpression: !Ref Schedule
      State: ENABLED
      FlexibleTimeWindow:
        Mode: 'OFF'
      Target:
          Arn: !GetAtt ModuleStepFunction.Arn
          RoleArn: !Ref SchedulerExecutionRoleARN

  LambdaAnalyticsExecutor:
    Type: Custom::LambdaAnalyticsExecutor
    Properties:
      ServiceToken: !Ref LambdaAnalyticsARN
      Name: !Ref CFDataName