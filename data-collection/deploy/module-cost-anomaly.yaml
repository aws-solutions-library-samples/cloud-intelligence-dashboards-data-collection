AWSTemplateFormatVersion: "2010-09-09"
Description: Retrieves AWS Cost Explorer Cost Anomalies details accross AWS organization
Parameters:
  DatabaseName:
    Type: String
    Description: Name of the Athena database to be created to hold lambda information
    Default: optimization_data
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket that exists or needs to be created to hold costanomaly information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  DestinationBucketARN:
    Type: String
    Description: ARN of the S3 Bucket that exists or needs to be created to hold costanomaly information
  ManagementRoleName:
    Type: String
    Description: The name of the IAM role that will be deployed in the management account which can retrieve AWS Organization data. KEEP THE SAME AS WHAT IS DEPLOYED INTO MANAGEMENT ACCOUNT
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: cost-anomaly
  GlueRoleARN:
    Type: String
    Description: Arn for the Glue Crawler role
  Schedule:
    Type: String
    Description: EventBridge Schedule to trigger the data collection
    Default: "rate(14 days)"
  ResourcePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  LambdaAnalyticsARN:
    Type: String
    Description: Arn of lambda for Analytics
  AccountCollectorLambdaARN:
    Type: String
    Description: Arn of the Account Collector Lambda
  StepFunctionTemplate:
    Type: String
    Description: JSON representation of common StepFunction template
  StepFunctionExecutionRoleARN:
    Type: String
    Description: Common role for Step Function execution
  SchedulerExecutionRoleARN:
    Type: String
    Description: Common role for module Scheduler execution

Outputs:
  StepFunctionARN:
    Description: ARN for the module's Step Function
    Value: !GetAtt ModuleStepFunction.Arn

Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${ResourcePrefix}${CFDataName}-LambdaRole"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Path: /
      Policies:
        - PolicyName: !Sub "${CFDataName}-ManagementAccount-LambdaRole"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: !Sub "arn:aws:iam::*:role/${ManagementRoleName}" # Need to assume a Read role in all Management accounts
        - PolicyName: "S3-Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:PutObject"
                  - "s3:GetObject"
                  - "s3:PutObjectAcl"
                Resource:
                  - !Sub "${DestinationBucketARN}/*"
              - Effect: "Allow"
                Action:
                  - "s3:ListBucket"
                Resource:
                  - !Sub "${DestinationBucketARN}"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ResourcePrefix}${CFDataName}-Lambda'
      Description: !Sub "Lambda function to retrieve ${CFDataName}"
      Runtime: python3.10
      Architectures: [x86_64]
      Code:
        ZipFile: |
          import os
          import json
          import logging
          from datetime import date, datetime, timedelta

          import boto3
          import botocore

          class ParamsBase():
              def __init__(self, env, additional_params):
                  logger.setLevel(getattr(logging, env.get('LOG_LEVEL', 'INFO').upper(), logging.INFO))
                  logger.debug("Loading parameters")
                  self.collection_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                  self.tmp_file = "/tmp/tmp.json"
                  self.bucket = env["BUCKET_NAME"]
                  self.role_name = env["ROLE_NAME"]
                  self.module_name = env["MODULE_NAME"]
                  self.regions = [r.strip() for r in env.get("REGIONS","us-east-1").split(',') if r]
                  self.boto_config = None
                  self.additional_params = additional_params

          """
          Custom implementation:
          These functions are specific to this module.
          """
          class Params(ParamsBase):
              """ Tailor this class to add any unique configuration """
              def __init__(self, env, additional_params):
                  try:
                      super().__init__(env, additional_params)
                  except (KeyError, AttributeError) as exc:
                      raise CidCriticalError(f"Invalid parameters supplied", exc)

          def get_api_data(account, params, region): #pylint: disable=unused-argument
              """ Tailor this method to call the necessary APIs and process the data """
              logger.debug(f"Entering get_api_data for region '{region}'")
              client = get_client_with_role(params.role_name, account.account_id, region=region, service="ce", params=params)
              results = []
              api_params = get_api_params(params=params)
              while True:
                  response = client.get_anomalies(**api_params)
                  for record in response['Anomalies']:
                      output = parse_record(record)
                      results.append(output)
                  if 'NextPageToken' in response:
                      api_params['NextPageToken'] = response['NextPageToken']
                  else:
                      break
              logger.info(f"API results total {len(results)}")
              return results

          def get_s3_key(account, params, region): #pylint: disable=unused-argument #using current live
              """ Tailor this method to set the appropriate S3 prefix and object name """
              return datetime.now().strftime(
                  f"{params.module_name}/{params.module_name}-data/payer_id={account.payer_id}/"
                  f"year=%Y/month=%m/day=%d/%Y-%m-%d.json")

          def get_api_params(account=None, params=None):
              """ Helper to add date bounding parameters """
              logger.debug("Entering get_api_params")
              start_date, end_date = calculate_dates(params.bucket, s3_path=f'{params.module_name}/{params.module_name}-data/')
              api_params = dict(
                  DateInterval={
                  'StartDate': str(start_date),
                  'EndDate': str(end_date)
                  },
                  MaxResults=100,
              )
              logger.info(f"Using the date range of: start_date={start_date}, end_date={end_date}")
              return api_params

          def parse_record(record):
              """ Creates a resulting record to store based on data from teh API call """
              logger.debug(f"Entering parse_record for {record}")
              try:
                  result = {
                      'AnomalyId': get_value_by_path(record, 'AnomalyId'),
                      'AnomalyStartDate': get_value_by_path(record, 'AnomalyStartDate'),
                      'AnomalyEndDate': get_value_by_path(record, 'AnomalyEndDate'),
                      'DimensionValue': get_value_by_path(record, 'DimensionValue'),
                      'MaxImpact': get_value_by_path(record, 'Impact/MaxImpact'),
                      'TotalActualSpend': get_value_by_path(record, 'Impact/TotalActualSpend'),
                      'TotalExpectedSpend': get_value_by_path(record, 'Impact/TotalExpectedSpend'),
                      'TotalImpact': get_value_by_path(record, 'Impact/TotalImpact'),
                      'TotalImpactpercentage': get_value_by_path(record, 'Impact/TotalImpactPercentage', 0),
                      'MonitorArn': get_value_by_path(record, 'MonitorArn'),
                      'LinkedAccount': get_value_by_path(record, 'RootCauses/0/LinkedAccount'),
                      'LinkedAccountName': get_value_by_path(record, 'RootCauses/0/LinkedAccountName'),
                      'Region': get_value_by_path(record, 'RootCauses/0/Region'),
                      'Service': get_value_by_path(record, 'RootCauses/0/Service'),
                      'UsageType': get_value_by_path(record, 'RootCauses/0/UsageType')
                  }
                  logger.debug(f"Processing record complete")
                  return result

              except Exception as exc:
                  raise CidCriticalError(f"Unhandled exception in parse_record", exc)

          def calculate_dates(bucket, s3_path):
              """ Timeboxes the range of anomalies by seeking the most recent data collection date from the last 90 days """
              end_date = datetime.now().date()
              start_date = datetime.now().date() - timedelta(days=90) #Cost anomalies are available for last 90days
              # Check the create time of objects in the S3 bucket
              paginator = boto3.client('s3').get_paginator('list_objects_v2')
              contents = sum( [page.get('Contents', []) for page in paginator.paginate(Bucket=bucket, Prefix=s3_path)], [])
              last_modified_date = get_last_modified_date(contents)
              if last_modified_date and last_modified_date >= start_date:
                  start_date = last_modified_date
              return start_date, end_date

          def get_last_modified_date(contents):
              """ Helper for calculate_dates """
              last_modified_dates = [obj['LastModified'].date() for obj in contents]
              last_modified_dates_within_90_days = [date for date in last_modified_dates if date >= datetime.now().date() - timedelta(days=90)]
              if last_modified_dates_within_90_days:
                  return max(last_modified_dates_within_90_days)
              return None

          def get_value_by_path(data, path, default=None):
              """ Helper to traverse a json blob for a multi-level search and safely return a default value if the path is not found """
              keys = path.split("/")
              current = data
              for key in keys:
                  if isinstance(current, dict) and key in current:
                      logger.debug(f"Found key {key}")
                      current = current.get(key, default)
                  elif isinstance(current, list) and key.isdigit():
                      try:
                          current = current[int(key)]
                          logger.debug(f"Found index {key}")
                      except (IndexError):
                          logger.debug(f"Index value {key} within path {path} is not valid in get_value_by_path for data {data}, returning default of {default}")
                          return default
                  else:
                      logger.debug(f"Key value {key} within path {path} is not valid in get_value_by_path for data {data}, returning default of {default}")
                      return default
              return current

          """
          Common implementation:
          These functions and classes are structured for common usage
          patterns across different modules
          """
          logger = logging.getLogger()
          for h in logger.handlers:
              h.setFormatter(logging.Formatter("[%(levelname)s] %(message)s (%(aws_request_id)s)"))
          logger = logging.getLogger(__name__)
          logger.setLevel(logging.INFO)

          def lambda_handler(event, context): #pylint: disable=unused-argument
              """ Common structured entry for Lambda invocation. Core processing done in main()."""
              logger.info(f"Incoming event: {json.dumps(event)}")
              status_code = 500
              try:
                  main(json.loads(event["account"]), event.get('params', ''))
                  status_code = 200
              except KeyError as exc:
                  raise CidCriticalError(f"Account is not defined in the incoming event data. Please do not trigger this function manually. Use the corresponding {os.environ['MODULE_NAME']} state machine in Step Functions instead.")
              except CidNonCriticalError as exc:
                  status_code = 200
              except CidCriticalError as exc:
                  raise exc
              except Exception as exc:
                  raise CidCriticalError(f"(UnhandledExceptionError) in this module", exc)
              finally:
                  return {"statusCode": status_code}

          def main(account_json, additional_params=None):
              """ Method to orchestrate the retrieval and storage of API data """
              logger.debug(f"Entering main")
              try:
                  account = Account(account_json)
                  logger.info(f"Entering main for account: {account.account_id}")
                  params = Params(os.environ, additional_params)
                  for region in params.regions:
                      logger.info(f"Processing for region '{region}'")
                      records = get_api_data(account, params, region)
                      if len(records) > 0:
                          store_to_temp(records, params)
                          upload_to_s3(account, params, region)
                      else:
                          logger.info(f"No file uploaded for region '{region}'")
                  logger.info(f"Exiting main without error")
              except CidError as exc:
                  raise exc
              except (botocore.exceptions.ClientError, ClientAccessError) as exc:
                  raise CidCriticalError(f"Possible role misconfiguration for {params.role_name}", exc)
              except Exception as exc:
                  raise CidCriticalError(f"(UnhandledExceptionError)", exc)

          def store_to_temp(records, params):
              """ Takes the list of processed records and moves them to a temp file """
              logger.debug("Entering store_to_temp")
              count = 0
              try:
                  with open(params.tmp_file, "w", encoding='utf-8') as f:
                      for record in records:
                          f.write(json.dumps(
                              record,
                              default=lambda x: x.isoformat() if isinstance(x, (date, datetime)) else None)
                              + "\n"
                          )
                          count += 1

              except Exception as exc:
                  raise CidCriticalError(f"Unhandled exception in store_to_temp", exc)

              logger.info(f"Stored {count} record(s) in temp file")
              return count

          def upload_to_s3(account, params, region=None):
              """ Moves the processed API data from the temp file to the designated S3 bucket """
              logger.debug(f"Entering upload_to_s3 for bucket {params.bucket}")
              try:
                  key = get_s3_key(account, params, region)
                  boto3.client('s3').upload_file(params.tmp_file, params.bucket, key)
                  logger.info(f"Data stored to s3://{params.bucket}/{key}")
                  return True

              except Exception as exc:
                  raise CidCriticalError("Exception in upload_to_s3", exc)

          def get_client_with_role(role_name, account_id, service, region, params):
              """ Assumes the designated data gathering read-only role and instantiates a boto3 client with it """
              logger.debug(f"Entering get_client_with_role to get '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'")
              try:
                  credentials = boto3.client('sts').assume_role(
                      RoleArn=f"arn:aws:iam::{account_id}:role/{role_name}",
                      RoleSessionName="data_collection"
                  )['Credentials']
                  logger.debug("Successfully assumed role, now getting client")
                  client = boto3.client(
                      service,
                      region_name = region,
                      aws_access_key_id = credentials['AccessKeyId'],
                      aws_secret_access_key = credentials['SecretAccessKey'],
                      aws_session_token = credentials['SessionToken'],
                      config = params.boto_config
                  )
                  logger.info(f"Successfully created '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'")
                  return client

              except Exception as exc:
                  raise ClientAccessError(exc, role_name, account_id, service, region)

          # Helper classes
          class Account():
              def __init__(self, account_json: dict):
                  try:
                      self.account_id = account_json["account_id"]
                      self.account_name = account_json["account_name"]
                      self.payer_id = account_json["payer_id"]
                  except KeyError:
                      raise CidCriticalError(f"Invalid account data passed {account_json}")

          class CidError(Exception):
              def __init__(self, message="", exc=None):
                  try:
                      message = f"({type(exc).__name__}) exception. {message}" if exc else message
                      if type(self) == CidNonCriticalError.__class__:
                          logger.warning(message)
                      else:
                          logger.error(message)
                      super().__init__(message)
                  except Exception as exc:
                      pass
          class CidNonCriticalError(CidError):
              def __init__(self, message="", exc=None):
                  super().__init__(message, exc)
          class CidCriticalError(CidError):
              def __init__(self, message="", exc=None):
                  super().__init__(message, exc)
          class ClientAccessError(Exception):
              def __init__(self, exc, role_name, account_id, service, region):
                  message = f"({type(exc).__name__}) exception: '{exc}' when getting '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'"
                  logger.warning(message)
                  super().__init__(message)
      Handler: "index.lambda_handler"
      MemorySize: 2688
      Timeout: 600
      Role: !GetAtt LambdaRole.Arn
      Environment:
        Variables:
          LOG_LEVEL: 'INFO'
          BUCKET_NAME: !Ref DestinationBucket
          MODULE_NAME: !Ref CFDataName
          ROLE_NAME: !Ref ManagementRoleName
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"

  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Sub '${ResourcePrefix}${CFDataName}-Crawler'
      Role: !Ref GlueRoleARN
      DatabaseName: !Ref DatabaseName
      Targets:
        S3Targets:
          - Path: !Sub "s3://${DestinationBucket}/${CFDataName}/${CFDataName}-data/"

  ModuleStepFunction:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub '${ResourcePrefix}${CFDataName}-StateMachine'
      StateMachineType: STANDARD
      RoleArn: !Ref StepFunctionExecutionRoleARN
      DefinitionString: !Ref StepFunctionTemplate
      DefinitionSubstitutions:
        AccountCollectorLambdaARN: !Ref AccountCollectorLambdaARN
        ModuleLambdaARN: !GetAtt LambdaFunction.Arn
        Crawlers: !Sub '["${ResourcePrefix}${CFDataName}-Crawler"]'
        CollectionType: "Payers"
        Params: ''
        Module: !Ref CFDataName
        DeployRegion: !Ref AWS::Region
        Account: !Ref AWS::AccountId
        Prefix: !Ref ResourcePrefix

  ModuleRefreshSchedule:
    Type: 'AWS::Scheduler::Schedule'
    Properties:
      Description: !Sub 'Scheduler for the ODC ${CFDataName} module'
      Name: !Sub '${ResourcePrefix}${CFDataName}-RefreshSchedule'
      ScheduleExpression: !Ref Schedule
      State: ENABLED
      FlexibleTimeWindow:
        Mode: 'OFF'
      Target:
          Arn: !GetAtt ModuleStepFunction.Arn
          RoleArn: !Ref SchedulerExecutionRoleARN

  LambdaAnalyticsExecutor:
    Type: Custom::LambdaAnalyticsExecutor
    Properties:
      ServiceToken: !Ref LambdaAnalyticsARN
      Name: !Ref CFDataName