AWSTemplateFormatVersion: '2010-09-09'
Description: Retrieves Inventory data for the chosen service
Transform: 'AWS::LanguageExtensions'
Parameters:
  DatabaseName:
    Type: String
    Description: Name of the Athena database to be created to hold lambda information
    Default: optimization_data
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket to be created to hold data information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  DestinationBucketARN:
    Type: String
    Description: ARN of the S3 Bucket that exists or needs to be created to hold rightsizing information
  MultiAccountRoleName:
    Type: String
    Description: Name of the IAM role deployed in all accounts which can retrieve AWS Data.
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: inventory
  GlueRoleARN:
    Type: String
    Description: Arn for the Glue Crawler role
  Schedule:
    Type: String
    Description: EventBridge Schedule to trigger the data collection
    Default: "rate(14 days)"
  ResourcePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  RegionsInScope:
    Type: String
    Description: "Comma Delimited list of AWS regions from which data about resources will be collected. Example: us-east-1,eu-west-1,ap-northeast-1"
  LambdaAnalyticsARN:
    Type: String
    Description: Arn of lambda for Analytics
  AccountCollectorLambdaARN:
    Type: String
    Description: Arn of the Account Collector Lambda
  StepFunctionTemplate:
    Type: String
    Description: JSON representation of common StepFunction template
  StepFunctionExecutionRoleARN:
    Type: String
    Description: Common role for Step Function execution
  SchedulerExecutionRoleARN:
    Type: String
    Description: Common role for module Scheduler execution
  AwsObjects:
    Type: CommaDelimitedList
    Default: OpensearchDomains, ElasticacheClusters, RdsDbInstances, EBS, AMI, Snapshot, Ec2Instances, VpcInstances
    Description: Services for pulling price data

Mappings:
  ServicesMap:
    OpensearchDomains:
      path: opensearch-domains
    ElasticacheClusters:
      path: elasticache-clusters
    RdsDbInstances:
      path: rds-db-instances
    EBS:
      path: ebs
    AMI:
      path: ami
    Snapshot:
      path: snapshot
    Ec2Instances:
      path: ec2-instances
    VpcInstances:
      path: vpc

Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${ResourcePrefix}${CFDataName}-LambdaRole"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Path: /
      Policies:
        - PolicyName: "AssumeMultiAccountRole"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: !Sub "arn:aws:iam::*:role/${MultiAccountRoleName}"
        - PolicyName: "S3Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:PutObject"
                Resource:
                  - !Sub "${DestinationBucketARN}/*"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ResourcePrefix}${CFDataName}-Lambda'
      Description: !Sub "Lambda Function to retrieve ${CFDataName}"
      Runtime: python3.10
      Architectures: [x86_64]
      Code:
        ZipFile: |
          """
          Scan linked accounts and store instances info to s3 bucket
          Supported types: ebs, snapshots, ami, rds instances
          """
          import os
          import json
          import logging
          from datetime import date, datetime
          from functools import partial

          import boto3
          import botocore

          class ParamsBase():
              def __init__(self, env, additional_params):
                  logger.setLevel(getattr(logging, env.get('LOG_LEVEL', 'INFO').upper(), logging.INFO))
                  logger.debug("Loading parameters")
                  self.collection_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                  self.tmp_file = "/tmp/tmp.json"
                  self.bucket = env["BUCKET_NAME"]
                  self.role_name = env["ROLE_NAME"]
                  self.module_name = env["MODULE_NAME"]
                  self.regions = [r.strip() for r in env.get("REGIONS","us-east-1").split(',') if r]
                  self.boto_config = None
                  self.additional_params = additional_params

          """
          Custom implementation:
          These functions are specific to this module.
          """
          class Params(ParamsBase):
              """ Tailor this class to add any unique configuration """
              def __init__(self, env, additional_params):
                  try:
                      super().__init__(env, additional_params)
                      self.tag_list = [t for t in env.get("TRACKING_TAGS", "").split() if t]
                      self.use_tags = len(self.tag_list) > 0
                      self.resource_type = additional_params["ResourceType"]
                  except (KeyError, AttributeError) as exc:
                      raise CidCriticalError(f"Invalid parameters supplied", exc)

          def get_api_data(account, params, region): #pylint: disable=unused-argument
              """ Tailor this method to call the necessary APIs and process the data """
              logger.debug(f"Entering get_api_data for region '{region}'")
              results = []
              sub_modules = {
                  'opensearch-domains': opensearch_domains_scan, # special function for opensearch
                  'elasticache-clusters': partial(
                      paginated_scan,
                      service='elasticache',
                      function_name='describe_cache_clusters',
                      obj_name='CacheClusters'
                  ),
                  'rds-db-instances': partial(
                      paginated_scan,
                      service='rds',
                      function_name='describe_db_instances',
                      obj_name='DBInstances'
                  ),
                  'ebs': partial(
                      paginated_scan,
                      service='ec2',
                      function_name='describe_volumes'
                  ),
                  'ami': partial(
                      paginated_scan,
                      service='ec2',
                      function_name='describe_images',
                      api_params={'Owners': ['self']}
                  ),
                  'snapshot': partial(
                      paginated_scan,
                      service='ec2',
                      function_name='describe_snapshots',
                      api_params={'OwnerIds': ['self']}
                  ),
                  'ec2-instances': partial(
                      paginated_scan,
                      service='ec2',
                      function_name='describe_instances',
                      obj_name='Reservations/Instances'
                  ),
                  'vpc': partial(
                      paginated_scan,
                      service='ec2',
                      function_name='describe_vpcs',
                      obj_name='Vpcs'
                  )
              }
              func = sub_modules[params.resource_type]
              for counter, obj in enumerate(func(account=account, region=region, params=params), start=1):
                  obj['accountid'] = account.account_id
                  if params.use_tags and "Tags" in obj:
                      logger.debug(f"Tags enabled and found tags {obj['Tags']}")
                      for tag in obj["Tags"]:
                          if tag["Key"] in params.tag_list:
                              obj[f"tag_{tag['Key']}"] = tag["Value"]
                  obj['collection_date'] = params.collection_time
                  results.append(obj)
              logger.info(f"API results total {len(results)}")
              return results

          def get_s3_key(account, params, region): #pylint: disable=unused-argument
              """ Tailor this method to set the appropriate S3 prefix and object name """
              return datetime.now().strftime(
                  f"{params.module_name}/{params.module_name}-data/payer_id={account.payer_id}/"
                  f"year=%Y/month=%m/{account.account_id}-%Y%m%d-%H%M%S.json")

          def paginated_scan(service, account, function_name, region, params, api_params=None, obj_name=None):
              """ paginated scan """
              logger.debug(f"Entering paginated_scan with service='{service}' function_name='{function_name}' api_params='{api_params}'")
              obj_name = obj_name or function_name.split('_')[-1].capitalize()
              client = get_client_with_role(params.role_name, account.account_id, region=region, service=service, params=params)
              paginator = client.get_paginator(function_name)
              for page in paginator.paginate(**(api_params or {})):
                  objs = get_value_by_path(page, obj_name)
                  for obj in objs or []:
                      obj['region'] = region
                      yield obj
              logger.info(f"Completed paginated_scan with service='{service}' function_name='{function_name}' api_params='{api_params}'")

          def opensearch_domains_scan(account, region, params, service="opensearch"):
              """ special treatment for opensearch_scan """
              logger.debug(f"Entering opensearch_domains_scan with service='{service}'")
              client = get_client_with_role(params.role_name, account.account_id, region=region, service=service, params=params)
              domain_names = [name.get('DomainName') for name in client.list_domain_names().get('DomainNames', [])]
              for domain_name in domain_names:
                  logger.debug(f"Processing domain '{domain_name}'")
                  client = get_client_with_role(params.role_name, account.account_id, region=region, service=service, params=params)
                  domain = client.describe_domain(DomainName=domain_name)['DomainStatus']
                  yield {
                      'region': region,
                      'DomainName': domain['DomainName'],
                      'DomainId': domain['DomainId'],
                      'EngineVersion': domain['EngineVersion'],
                      'InstanceType': domain['ClusterConfig']['InstanceType'],
                      'InstanceCount': domain['ClusterConfig']['InstanceCount'],
                  }
              logger.info(f"Completed opensearch_domains_scan")

          def get_value_by_path(data, path):
              keys = path.split("/")
              current = data
              for key in keys:
                  while True:
                      if isinstance(current, dict) and key in current:
                          current = current[key]
                          break
                      elif isinstance(current, list):
                          try:
                              index = int(key) if isinstance(key, int) else 0
                              current = current[index]
                          except (ValueError, IndexError):
                              return None
                      else:
                          return None
              return current

          """
          Common implementation:
          These functions and classes are structured for common usage
          patterns across different modules
          """
          logger = logging.getLogger()
          for h in logger.handlers:
              h.setFormatter(logging.Formatter("[%(levelname)s] %(message)s (%(aws_request_id)s)"))
          logger = logging.getLogger(__name__)
          logger.setLevel(logging.INFO)

          def lambda_handler(event, context): #pylint: disable=unused-argument
              """ Common structured entry for Lambda invocation. Core processing done in main()."""
              logger.info(f"Incoming event: {json.dumps(event)}")
              status_code = 500
              try:
                  main(json.loads(event["account"]), event.get('params', ''))
                  status_code = 200
              except KeyError as exc:
                  raise CidCriticalError(f"Account is not defined in the incoming event data. Please do not trigger this function manually. Use the corresponding {os.environ['MODULE_NAME']} state machine in Step Functions instead.")
              except CidNonCriticalError as exc:
                  status_code = 200
              except CidCriticalError as exc:
                  raise exc
              except Exception as exc:
                  raise CidCriticalError(f"(UnhandledExceptionError) in this module", exc)
              finally:
                  return {"statusCode": status_code}

          def main(account_json, additional_params=None):
              """ Method to orchestrate the retrieval and storage of API data """
              logger.debug(f"Entering main")
              try:
                  account = Account(account_json)
                  logger.info(f"Entering main for account: {account.account_id}")
                  params = Params(os.environ, additional_params)
                  for region in params.regions:
                      logger.info(f"Processing for region '{region}'")
                      records = get_api_data(account, params, region)
                      if len(records) > 0:
                          store_to_temp(records, params)
                          upload_to_s3(account, params, region)
                      else:
                          logger.info(f"No file uploaded for region '{region}'")
                  logger.info(f"Exiting main without error")
              except CidError as exc:
                  raise exc
              except (botocore.exceptions.ClientError, ClientAccessError) as exc:
                  raise CidCriticalError(f"Possible role misconfiguration for {params.role_name}", exc)
              except Exception as exc:
                  raise CidCriticalError(f"(UnhandledExceptionError)", exc)

          def store_to_temp(records, params):
              """ Takes the list of processed records and moves them to a temp file """
              logger.debug("Entering store_to_temp")
              count = 0
              try:
                  with open(params.tmp_file, "w", encoding='utf-8') as f:
                      for record in records:
                          f.write(json.dumps(
                              record,
                              default=lambda x: x.isoformat() if isinstance(x, (date, datetime)) else None)
                              + "\n"
                          )
                          count += 1

              except Exception as exc:
                  raise CidCriticalError(f"Unhandled exception in store_to_temp", exc)

              logger.info(f"Stored {count} record(s) in temp file")
              return count

          def upload_to_s3(account, params, region=None):
              """ Moves the processed API data from the temp file to the designated S3 bucket """
              logger.debug(f"Entering upload_to_s3 for bucket {params.bucket}")
              try:
                  key = get_s3_key(account, params, region)
                  boto3.client('s3').upload_file(params.tmp_file, params.bucket, key)
                  logger.info(f"Data stored to s3://{params.bucket}/{key}")
                  return True

              except Exception as exc:
                  raise CidCriticalError("Exception in upload_to_s3", exc)

          def get_client_with_role(role_name, account_id, service, region, params):
              """ Assumes the designated data gathering read-only role and instantiates a boto3 client with it """
              logger.debug(f"Entering get_client_with_role to get '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'")
              try:
                  credentials = boto3.client('sts').assume_role(
                      RoleArn=f"arn:aws:iam::{account_id}:role/{role_name}",
                      RoleSessionName="data_collection"
                  )['Credentials']
                  logger.debug("Successfully assumed role, now getting client")
                  client = boto3.client(
                      service,
                      region_name = region,
                      aws_access_key_id = credentials['AccessKeyId'],
                      aws_secret_access_key = credentials['SecretAccessKey'],
                      aws_session_token = credentials['SessionToken'],
                      config = params.boto_config
                  )
                  logger.info(f"Successfully created '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'")
                  return client

              except Exception as exc:
                  raise ClientAccessError(exc, role_name, account_id, service, region)

          # Helper classes
          class Account():
              def __init__(self, account_json: dict):
                  try:
                      self.account_id = account_json["account_id"]
                      self.account_name = account_json["account_name"]
                      self.payer_id = account_json["payer_id"]
                  except KeyError:
                      raise CidCriticalError(f"Invalid account data passed {account_json}")

          class CidError(Exception):
              def __init__(self, message="", exc=None):
                  try:
                      message = f"({type(exc).__name__}) exception. {message}" if exc else message
                      if type(self) == CidNonCriticalError.__class__:
                          logger.warning(message)
                      else:
                          logger.error(message)
                      super().__init__(message)
                  except Exception as exc:
                      pass
          class CidNonCriticalError(CidError):
              def __init__(self, message="", exc=None):
                  super().__init__(message, exc)
          class CidCriticalError(CidError):
              def __init__(self, message="", exc=None):
                  super().__init__(message, exc)
          class ClientAccessError(Exception):
              def __init__(self, exc, role_name, account_id, service, region):
                  message = f"({type(exc).__name__}) exception: '{exc}' when getting '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'"
                  logger.warning(message)
                  super().__init__(message)
      Handler: 'index.lambda_handler'
      MemorySize: 2688
      Timeout: 300
      Role: !GetAtt LambdaRole.Arn
      Environment:
        Variables:
          LOG_LEVEL: 'INFO'
          BUCKET_NAME: !Ref DestinationBucket
          MODULE_NAME: !Ref CFDataName
          ROLE_NAME: !Ref MultiAccountRoleName
          REGIONS: !Ref RegionsInScope

  'Fn::ForEach::Object':
    - AwsObject
    - !Ref AwsObjects
    - 'Crawler${AwsObject}':
        Type: AWS::Glue::Crawler
        Properties:
          Name: !Sub '${ResourcePrefix}${CFDataName}-${AwsObject}-Crawler'
          Role: !Ref GlueRoleARN
          DatabaseName: !Ref DatabaseName
          Targets:
            S3Targets:
              - Path:
                  Fn::Sub:
                    - "s3://${DestinationBucket}/inventory/inventory-${path}-data/"
                    - path: !FindInMap [ServicesMap, !Ref AwsObject, path]
          Configuration: |
            {
              "Version": 1.0,
              "CrawlerOutput": {
                "Partitions": {
                  "AddOrUpdateBehavior": "InheritFromTable"
                }
              }
            }
      'StepFunction${AwsObject}':
        Type: AWS::StepFunctions::StateMachine
        Properties:
          StateMachineName: !Sub '${ResourcePrefix}${CFDataName}-${AwsObject}-StateMachine'
          StateMachineType: STANDARD
          RoleArn: !Ref StepFunctionExecutionRoleARN
          DefinitionString: !Ref StepFunctionTemplate
          DefinitionSubstitutions:
            AccountCollectorLambdaARN: !Ref AccountCollectorLambdaARN
            ModuleLambdaARN: !GetAtt LambdaFunction.Arn
            Crawlers: !Sub '["${ResourcePrefix}${CFDataName}-${AwsObject}-Crawler"]'
            CollectionType: "LINKED"
            Params: !Sub
              - '"ResourceType":"${ResourceType}"'
              - ResourceType: !FindInMap [ServicesMap, !Ref AwsObject, path]
            Module: !Ref CFDataName
            DeployRegion: !Ref AWS::Region
            Account: !Ref AWS::AccountId
            Prefix: !Ref ResourcePrefix
      'RefreshSchedule${AwsObject}':
        Type: AWS::Scheduler::Schedule
        Properties:
          Description: !Sub 'Scheduler for the ODC ${CFDataName} ${AwsObject} module'
          Name: !Sub '${ResourcePrefix}${CFDataName}-${AwsObject}-RefreshSchedule'
          ScheduleExpression: !Ref Schedule
          State: ENABLED
          FlexibleTimeWindow:
            Mode: 'OFF'
          Target:
              Arn: !GetAtt [!Sub 'StepFunction${AwsObject}', Arn]
              RoleArn: !Ref SchedulerExecutionRoleARN

  LambdaAnalyticsExecutor:
    Type: Custom::LambdaAnalyticsExecutor
    Properties:
      ServiceToken: !Ref LambdaAnalyticsARN
      Name: !Ref CFDataName

  AthenaSnaphotAMI:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Identifies snapshots connected to AMI's
      Name: inventory_snapshot_connected_to_ami
      QueryString: |
        SELECT distinct(snapshotid),volume,volumesize,starttime,snapdescription, ownerid,
        snap_ami_id, imageid, name, description, state, rootdevicetype, virtualizationtype,year, month,
        CASE
        WHEN snap_ami_id = imageid THEN 'AMI Avalible'
        WHEN snap_ami_id LIKE 'ami%' THEN 'AMI Removed'
        ELSE 'Not AMI'
        END AS status
          FROM (
        (SELECT snapshotid,
            volumeid as volume,
            volumesize,
            starttime,
            Description AS snapdescription,
            year,
            month,
            ownerid,

            CASE
            WHEN substr(Description, 1, 22) = 'Created by CreateImage' THEN
            split_part(Description,' ', 5)
            WHEN substr(Description, 2, 11) = 'Copied snap' THEN
            split_part(Description,' ', 9)
            WHEN substr(Description, 1, 22) = 'Copied for Destination' THEN
            split_part(Description,' ', 4)
            ELSE ''
            END AS snap_ami_id
        FROM optimization_data.inventory_snapshot_data
        ) AS snapshots
        LEFT JOIN
            (SELECT imageid,
            name,
            description,
            state,
            rootdevicetype,
            virtualizationtype
            FROM optimization_data.inventory_ami_data) AS ami
                ON snapshots.snap_ami_id = ami.imageid )

  AthenaSnaphotAMICUR:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Identifies snapshots connected to AMI's with CUR data
      Name: inventory_snapshot_connected_to_ami_with_CUR
      QueryString: |
          SELECT DISTINCT
            snapshotid
          , volume
          , volumesize
          , starttime
          , snapdescription
          , ownerid
          , snap_ami_id
          , imageid
          , name
          , description
          , state
          , rootdevicetype
          , virtualizationtype
          , snapshots.year
          , snapshots.month
          , (CASE WHEN (snap_ami_id = imageid) THEN 'AMI Avalible' WHEN (snap_ami_id LIKE 'ami%') THEN 'AMI Removed' ELSE 'Not AMI' END) status
          ,sum_line_item_usage_amount
          ,sum_line_item_unblended_cost
          FROM
          (SELECT
              snapshotid
            , volumeid volume
            , volumesize
            , starttime
            , Description snapdescription
            , inventory_snapshot_data.year
            , inventory_snapshot_data.month
            , ownerid
            , (CASE WHEN ("substr"(Description, 1, 22) = 'Created by CreateImage') THEN "split_part"(Description, ' ', 5) WHEN ("substr"(Description, 2, 11) = 'Copied snap') THEN "split_part"(Description, ' ', 9) WHEN ("substr"(Description, 1, 22) = 'Copied for Destination') THEN "split_part"(Description, ' ', 4) ELSE '' END) snap_ami_id
            FROM
              optimization_data.inventory_snapshot_data)
          as   snapshots
          LEFT JOIN (
            SELECT
              imageid
            , name
            , description
            , state
            , rootdevicetype
            , virtualizationtype
            FROM
              optimization_data.inventory_ami_data
          )  ami ON (snapshots.snap_ami_id = ami.imageid)
          left join
          (SELECT
                bill_payer_account_id,
                line_item_usage_account_id,
                product_region,
                split("line_item_resource_id", '/')[2]resource_id,
                ${table_name}.month,
                ${table_name}.year,
                SUM(CAST(line_item_usage_amount AS DOUBLE)) AS sum_line_item_usage_amount,
                SUM(CAST(line_item_unblended_cost AS DECIMAL(16,8))) AS sum_line_item_unblended_cost
              FROM
                ${database}.${table_name}
              WHERE
                ${date_filter}
                AND product_product_name = 'Amazon Elastic Compute Cloud'
                AND line_item_usage_type LIKE '%%EBS%%Snapshot%%'
                AND product_product_family LIKE 'Storage Snapshot'
                AND line_item_line_item_type  IN ('DiscountedUsage', 'Usage', 'SavingsPlanCoveredUsage')
              GROUP BY
                bill_payer_account_id,
                line_item_usage_account_id,
                product_region,
                "line_item_resource_id",month, year
              ORDER BY
                sum_line_item_unblended_cost DESC,
                sum_line_item_usage_amount DESC
          ) as cur
          on snapshots.snapshotid = cur.resource_id
          and snapshots.ownerid=cur.line_item_usage_account_id
          and snapshots.month=cur.month
          and snapshots.year=cur.year
          where snapshots.month = '11' and snapshots.year= '2022'

  AthenaSnaphotAMIPricing:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Identifies snapshots connected to AMI's with Pricing data
      Name: inventory_snapshot_connected_to_ami_with_pricing
      QueryString: |
        CREATE OR REPLACE VIEW snapshot_ami_quicksight_view AS
        SELECT *,
          CASE
            WHEN snap_ami_id = imageid THEN 'AMI Avalible'
            WHEN snap_ami_id LIKE 'ami%' THEN 'AMI Removed' ELSE 'Not AMI'
          END AS status,
         cast(priceperunit as decimal)*volumesize as est_snap_cost
        FROM ((((
            (
              SELECT snapshotid AS snap_id,
                volumeid as volume,
                volumesize,
                starttime,
                Description AS snapdescription,
                year,
                month,
                region,
                ownerid,
                CASE
                  WHEN substr(Description, 1, 22) = 'Created by CreateImage' THEN split_part(Description, ' ', 5)
                  WHEN substr(Description, 2, 11) = 'Copied snap' THEN split_part(Description, ' ', 9)
                  WHEN substr(Description, 1, 22) = 'Copied for Destination' THEN split_part(Description, ' ', 4) ELSE ''
                END AS snap_ami_id
              FROM optimization_data.inventory_snapshot_data
            ) AS snapshots
            LEFT JOIN (
              SELECT imageid,
                name,
                description,
                state,
                rootdevicetype,
                virtualizationtype,
                month as ami_month,
                year as ami_year
              FROM optimization_data.inventory_ami_data) AS ami
            ON snapshots.snap_ami_id = ami.imageid and snapshots.month=ami.ami_month and snapshots.year = ami.ami_year
          )
          LEFT JOIN (
          SELECT
            region region_code
          , regionname
          FROM
            optimization_data.pricing_regionnames_data
        )  region ON (snapshots.region = region.region_code))
        LEFT JOIN (
          SELECT DISTINCT
            volumeid volume_mapping
          , count(DISTINCT snapshotid) snap_count
          FROM
            optimization_data.inventory_snapshot_data
          GROUP BY 1
        )  mapping ON (mapping.volume_mapping = snapshots.volume)) s
        LEFT JOIN (
          SELECT
            "productfamily"
          , location
          , priceperunit
          , unit
          FROM
            optimization_data.pricing_ec2_data
          WHERE ((("productfamily" = 'Storage Snapshot') AND (usagetype LIKE '%SnapshotUsage%')) AND ("locationtype" = 'AWS Region'))
        )  snap_unit_price ON (s.regionname = snap_unit_price.location))

  AthenaEBSTA:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Identifies snapshots connected to AMI's with Pricing data
      Name: inventory_ebs_with_ta_data
      QueryString: |
        SELECT *
        FROM optimization_data.inventory_ebs_data
        LEFT JOIN (
          SELECT "volume id","volume name", "volume type","volume size",status,	"monthly storage cost" ,accountid as taaccountid, category, region as taregion, year as tayear,month as tamonth
          FROM optimization_data.trusted_advisor_data
          WHERE category = 'cost_optimizing' ) ta
          ON inventory_ebs_data.volumeid = ta."volume id" and inventory_ebs_data.year = ta.tayear and inventory_ebs_data.month = ta.tamonth
        LEFT JOIN (
          SELECT
            region region_code
          , regionname
          FROM optimization_data.pricing_regionnames_data
          )  region
          ON (inventory_ebs_data.region = region.region_code)

  AthenaEBSView:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Identifies snapshots connected to AMI's with Pricing data
      Name: ebs-view
      QueryString: |
        CREATE OR REPLACE VIEW inventory_ebs_view AS
        SELECT * FROM
                    optimization_data.inventory_ebs_data
                LEFT JOIN
                (select "volume id","volume name", "volume type","volume size",	"monthly storage cost" ,accountid as ta_accountid, status, category, region as ta_region, year as ta_year ,month as ta_month
                from
                optimization_data.trusted_advisor_data
                where category = 'cost_optimizing') ta
                ON inventory_ebs_data.volumeid = ta."volume id" and inventory_ebs_data.year = ta.ta_year and inventory_ebs_data.month = ta.ta_month
                LEFT JOIN (
          SELECT
            "region" "region_code"
          , regionname
          FROM
            optimization_data.pricing_regionnames_data
        )  region ON (inventory_ebs_data.region = region.region_code)

  AthenaEBSTAPricing:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Identifies snapshots connected to AMI's with Pricing data
      Name: inventory_ebs_with_ta_data_with_pricing
      QueryString: |
        CREATE OR REPLACE VIEW ebs_quicksight_view AS
          SELECT
            volumeid
          , volumetype
          , size
          , attachments
          , iops
          , region
          , accountid
          , TRY(date_parse(createtime, '%Y-%m-%d %H:%i:%s')) date_created
          , year
          , month
          , CAST(concat(year, '-', month, '-01') AS date) billing_period
          , concat(year, '-', month) date
          , (CASE status WHEN 'warning' THEN 'Underutilised' ELSE 'Healthy' END) status
          , priceperunit
          , iop_price
          , (priceperunit * size) ebs_gb_cost
          , (priceperunit * size) ebs_cost
          , (CASE WHEN (volumetype = 'io1') THEN (iop_price * CAST(iops AS integer)) WHEN (volumetype = 'io2') THEN (iop_price * CAST(iops AS integer)) ELSE 0 END) iop_cost
          FROM
            ((optimization_data.inventory_ebs_view
          LEFT JOIN (
            SELECT
              "volume api name"
            , location
            , priceperunit
            , unit
            , "product family"
            FROM
              optimization_data.pricing_ec2_data
            WHERE ("product family" = 'Storage')
          )  ebs_price ON ((inventory_ebs_view.volumetype = ebs_price."volume api name") AND (inventory_ebs_view.regionname = ebs_price.location)))
          LEFT JOIN (
            SELECT
              "volume api name"
            , location
            , priceperunit iop_price
            , unit
            , "product family"
            , usagetype
            FROM
              optimization_data.pricing_ec2_data
            WHERE ((("Product Family" = 'System Operation') AND ("volume api name" IN ('io1', 'io2'))) AND ((usagetype LIKE '%EBS:VolumeP-IOPS.io2') OR (usagetype LIKE '%EBS:VolumeP-IOPS.piops')))
          )  ebs_iops_price ON ((inventory_ebs_view.volumetype = ebs_iops_price."volume api name") AND (inventory_ebs_view.regionname = ebs_iops_price.location)))
          ORDER BY volumeid DESC

  AthenaGp3Opportunity:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Identifies gp2, io1, io2 that could move to gp3
      Name: gp3-opportunity
      QueryString: |
        WITH raw_ebs_pricedata AS (
        SELECT
            region,
            map_agg(
                CASE WHEN "volume api name" = 'io1' AND unit = 'GB-Mo' THEN 'io1_GB-Mo'
                    WHEN "volume api name" = 'io2' AND unit = 'GB-month' THEN 'io2_GB-Mo'
                    WHEN "volume api name" = 'gp3' AND unit = 'GB-Mo' THEN 'gp3_GB-Mo'
                    WHEN "volume api name" = 'gp2' AND unit = 'GB-Mo' THEN 'gp2_GB-Mo'
                    WHEN "volume api name" = 'io1' AND unit = 'IOPS-Mo' THEN 'io1_IOPS-Mo'
                    WHEN "volume api name" = 'io2' AND unit = 'IOPS-Mo' AND "group" = 'EBS IOPS' THEN 'io2_IOPS-Mo_tier1'
                    WHEN "volume api name" = 'io2' AND unit = 'IOPS-Mo' AND "group" = 'EBS IOPS Tier 2' THEN 'io2_IOPS-Mo_tier2'
                    WHEN "volume api name" = 'io2' AND unit = 'IOPS-Mo' AND "group" = 'EBS IOPS Tier 3' THEN 'io2_IOPS-Mo_tier3'
                    WHEN "volume api name" = 'gp3' AND unit = 'IOPS-Mo' THEN 'gp3_IOPS-Mo'
                    WHEN "volume api name" = 'gp3' AND unit = 'GiBps-mo' THEN 'gp3_GiBps-mo'
                    ELSE unit
                END, priceperunit) kv1
        FROM optimization_data.pricing_ec2_data
        JOIN optimization_data.pricing_regionnames_data ON pricing_regionnames_data.regionname = pricing_ec2.location
        WHERE "volume api name" IN ('io1','io2','gp3','gp2')
        GROUP BY region
        ),
        pivoted_ebs_pricedata AS (
            SELECT region,
                  kv1['io1_GB-Mo'] as io1_gb_unit_cost,
                  kv1['io2_GB-Mo'] as io2_gb_unit_cost,
                  kv1['gp2_GB-Mo'] as gp2_gb_unit_cost,
                  kv1['gp3_GB-Mo'] AS gp3_gb_unit_cost,
                  kv1['io1_IOPS-Mo'] as io1_iops_unit_cost,
                  kv1['io2_IOPS-Mo_tier1'] as io2_iops_t1_unit_cost,
                  kv1['io2_IOPS-Mo_tier2'] as io2_iops_t2_unit_cost,
                  kv1['io2_IOPS-Mo_tier3'] as io2_iops_t3_unit_cost,
                  kv1['gp3_IOPS-Mo'] as gp3_iops_unit_cost,
                  kv1['gp3_GiBps-mo']/1000. AS gp3_throughput_unit_cost
            FROM raw_ebs_pricedata
        ),
        ebs_data AS (
            SELECT
                volumeid,
                "volume name",
                "volume type",
                size,
                attachments,
                iops,
                inventory_ebs_view.region,
                accountid,
                year,
                month,
                CASE status
                    WHEN 'warning' THEN 'Underutilised'
                    ELSE 'Healthy'
                END AS status,
                CASE
                    WHEN volumetype = 'io1' THEN io1_gb_unit_cost
                    WHEN volumetype = 'io2' THEN io2_gb_unit_cost
                    WHEN volumetype = 'gp2' THEN gp2_gb_unit_cost
                END as gb_unit_cost,
                CASE
                    WHEN volumetype = 'io1' THEN io1_gb_unit_cost * size
                    WHEN volumetype = 'io2' THEN io2_gb_unit_cost * size
                    WHEN volumetype = 'gp2' THEN gp2_gb_unit_cost * size
                END as current_gb_cost,
                CASE
                    WHEN volumetype = 'io1' THEN io1_iops_unit_cost
                    WHEN volumetype = 'io2' THEN io2_iops_t1_unit_cost
                    WHEN volumetype = 'gp2' THEN 0.0
                END as iops_unit_cost,
                CASE
                    WHEN volumetype = 'io1' THEN io1_iops_unit_cost * CAST(iops AS double)
                    WHEN volumetype = 'io2' THEN io2_iops_t1_unit_cost * CAST(iops AS double)
                    WHEN volumetype = 'gp2' THEN 0.0
                END as current_iops_cost,
                gp3_gb_unit_cost,
                gp3_gb_unit_cost * size AS gp3_gb_cost,
                gp3_iops_unit_cost,
                CASE
                    WHEN cast(iops as integer) <= 3000 THEN 0.0
                    ELSE (CAST(iops AS double) - 3000.) * gp3_iops_unit_cost
                END as gp3_iops_cost,
                gp3_throughput_unit_cost,
                CASE
                    WHEN volumetype = 'io1' THEN gp3_throughput_unit_cost * 500
                    WHEN volumetype = 'io2' THEN gp3_throughput_unit_cost * 500
                    WHEN volumetype = 'gp2' AND size <= 170 THEN gp3_throughput_unit_cost * (128.0 - 125.0)
                    WHEN volumetype = 'gp2' AND size > 170 THEN gp3_throughput_unit_cost * (250.0 - 125.0)
                END AS gp3_throughput_cost
            FROM optimization_data.inventory_ebs_view
            LEFT JOIN pivoted_ebs_pricedata ON pivoted_ebs_pricedata.region = inventory_ebs_view.region
            WHERE
                volumetype in('io1', 'io2', 'gp2')
                and cast(iops as integer) < 16000 )
          SELECT
              *,
              current_gb_cost + current_iops_cost AS current_total_cost,
              gp3_gb_cost + gp3_iops_cost + gp3_throughput_cost AS gp3_total_cost,
              (current_gb_cost + current_iops_cost) - (gp3_gb_cost + gp3_iops_cost + gp3_throughput_cost) as gp3_saving
          FROM inventory_ebs_data

  AthenaBackwardCompatPricingRegionNames:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Use as needed if queries were previously dependent upon pricing_region_names, which has been renamed for standardization
      Name: backward_compat_pricing_region_names
      QueryString: |
        CREATE OR REPLACE VIEW pricing_region_names AS SELECT * FROM optimization_data.pricing_regionnames_data