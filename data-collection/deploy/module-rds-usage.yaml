AWSTemplateFormatVersion: '2010-09-09'
Description: Retrieves RDS Metric data
Parameters:
  DatabaseName:
    Type: String
    Description: Name of the Athena database to be created to hold lambda information
    Default: optimization_data
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket to be created to hold data information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  DestinationBucketARN:
    Type: String
    Description: ARN of the S3 Bucket that exists or needs to be created to hold rightsizing information
  MultiAccountRoleName:
    Type: String
    Description: Name of the IAM role deployed in all accounts which can retrieve AWS Data.
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: rds-usage
  GlueRoleARN:
    Type: String
    Description: Arn for the Glue Crawler role
  Schedule:
    Type: String
    Description: EventBridge Schedule to trigger the data collection
    Default: "rate(14 days)"
  ResourcePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  RegionsInScope:
    Type: String
    Description: "Comma Delimited list of AWS regions from which data about resources will be collected. Example: us-east-1,eu-west-1,ap-northeast-1"
  LambdaAnalyticsARN:
    Type: String
    Description: Arn of lambda for Analytics
  AccountCollectorLambdaARN:
    Type: String
    Description: Arn of the Account Collector Lambda
  StepFunctionTemplate:
    Type: String
    Description: JSON representation of common StepFunction template
  StepFunctionExecutionRoleARN:
    Type: String
    Description: Common role for Step Function execution
  SchedulerExecutionRoleARN:
    Type: String
    Description: Common role for module Scheduler execution
  DAYS:
    Type: Number
    Description: Number of days going back that you want to get data for
    Default: 1

Outputs:
  StepFunctionARN:
    Description: ARN for the module's Step Function
    Value: !GetAtt ModuleStepFunction.Arn

Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${ResourcePrefix}${CFDataName}-LambdaRole"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Path: /
      Policies:
        - PolicyName: "AssumeMultiAccountRole"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: !Sub "arn:aws:iam::*:role/${MultiAccountRoleName}"
        - PolicyName: "S3Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:PutObject"
                Resource:
                  - !Sub "${DestinationBucketARN}/*"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ResourcePrefix}${CFDataName}-Lambda'
      Description: !Sub "Lambda function to retrieve ${CFDataName}"
      Runtime: python3.10
      Architectures: [x86_64]
      Code:
        ZipFile: |
          import os
          import json
          import logging
          from datetime import date, datetime, timedelta

          import boto3
          import botocore

          class ParamsBase():
              def __init__(self, env, additional_params):
                  logger.setLevel(getattr(logging, env.get('LOG_LEVEL', 'INFO').upper(), logging.INFO))
                  logger.debug("Loading parameters")
                  self.collection_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                  self.tmp_file = "/tmp/tmp.json"
                  self.bucket = env["BUCKET_NAME"]
                  self.role_name = env["ROLE_NAME"]
                  self.module_name = env["MODULE_NAME"]
                  self.regions = [r.strip() for r in env.get("REGIONS","us-east-1").split(',') if r]
                  self.boto_config = None
                  self.additional_params = additional_params

          """
          Custom implementation:
          These functions are specific to this module.
          """
          class Params(ParamsBase):
              """ Tailor this class to add any unique configuration """
              def __init__(self, env, additional_params):
                  try:
                      super().__init__(env, additional_params)
                      self.days = int(env["DAYS"])
                      self.past = datetime.utcnow() - timedelta(days=self.days)
                      self.future = datetime.utcnow() # + timedelta(minutes=10)
                      self.period = 3600
                      self.metrics = [
                          'FreeableMemory',
                          'CPUUtilization',
                          'NetworkReceiveThroughput',
                          'NetworkTransmitThroughput',
                          'ReadIOPS',
                          'WriteIOPS',
                          'FreeStorageSpace'
                      ]
                  except (KeyError, AttributeError) as exc:
                      raise CidCriticalError(f"Invalid parameters supplied", exc)

          def get_api_data(account, params, region): #pylint: disable=unused-argument
              """ Tailor this method to call the necessary APIs and process the data """
              logger.debug(f"Entering get_api_data for region '{region}'")
              cw_client = get_client_with_role(params.role_name, account.account_id, region=region, service="cloudwatch", params=params)
              client = get_client_with_role(params.role_name, account.account_id, region=region, service="rds", params=params)
              results = []
              datapoints = {}
              rds_inventory = client.describe_db_instances()
              for rds in rds_inventory['DBInstances']:
                  for metric in params.metrics:
                      metric_data = cw_client.get_metric_statistics(StartTime=params.past, EndTime=params.future, MetricName=metric,
                          Namespace='AWS/RDS',Statistics=['Average','Maximum','Minimum'],Period=params.period,
                          Dimensions=[{"Name": "DBInstanceIdentifier", "Value": rds["DBInstanceIdentifier"]}])
                      datapoints.update({metric:metric_data['Datapoints']})
                  rds["Datapoints"] = datapoints
                  results.append(rds)

              logger.info(f"API results total {len(results)}")
              return results

          def get_s3_key(account, params, region): #pylint: disable=unused-argument
              """ Tailor this method to set the appropriate S3 prefix and object name """
              return datetime.now().strftime(
                  f"{params.module_name}/{params.module_name}-data/payer_id={account.payer_id}/"
                  f"year=%Y/month=%m/{params.module_name}-{region}-%Y-%m-%d.json")
                  #f"{params.module_name}/{params.module_name}-data/payer_id={account.payer_id}/"
                  #f"accountid={params.account_id}/region={region}/rds_id={resource_value}/"
                  #f"year=%Y/month=%m/%Y-%m-%d-rds_stats.json"

          """
          Common implementation:
          These functions and classes are structured for common usage
          patterns across different modules
          """
          logger = logging.getLogger()
          for h in logger.handlers:
              h.setFormatter(logging.Formatter("[%(levelname)s] %(message)s (%(aws_request_id)s)"))
          logger = logging.getLogger(__name__)
          logger.setLevel(logging.INFO)

          def lambda_handler(event, context): #pylint: disable=unused-argument
              """ Common structured entry for Lambda invocation. Core processing done in main()."""
              logger.info(f"Incoming event: {json.dumps(event)}")
              status_code = 500
              try:
                  main(json.loads(event["account"]), event.get('params', ''))
                  status_code = 200
              except KeyError as exc:
                  raise CidCriticalError(f"Account is not defined in the incoming event data. Please do not trigger this function manually. Use the corresponding {os.environ['MODULE_NAME']} state machine in Step Functions instead.")
              except CidNonCriticalError as exc:
                  status_code = 200
              except CidCriticalError as exc:
                  raise exc
              except Exception as exc:
                  raise CidCriticalError(f"(UnhandledExceptionError) in this module", exc)
              finally:
                  return {"statusCode": status_code}

          def main(account_json, additional_params=None):
              """ Method to orchestrate the retrieval and storage of API data """
              logger.debug(f"Entering main")
              try:
                  account = Account(account_json)
                  logger.info(f"Entering main for account: {account.account_id}")
                  params = Params(os.environ, additional_params)
                  for region in params.regions:
                      logger.info(f"Processing for region '{region}'")
                      records = get_api_data(account, params, region)
                      if len(records) > 0:
                          store_to_temp(records, params)
                          upload_to_s3(account, params, region)
                      else:
                          logger.info(f"No file uploaded for region '{region}'")
                  logger.info(f"Exiting main without error")
              except CidError as exc:
                  raise exc
              except (botocore.exceptions.ClientError, ClientAccessError) as exc:
                  raise CidCriticalError(f"Possible role misconfiguration for {params.role_name}", exc)
              except Exception as exc:
                  raise CidCriticalError(f"(UnhandledExceptionError)", exc)

          def store_to_temp(records, params):
              """ Takes the list of processed records and moves them to a temp file """
              logger.debug("Entering store_to_temp")
              count = 0
              try:
                  with open(params.tmp_file, "w", encoding='utf-8') as f:
                      for record in records:
                          f.write(json.dumps(
                              record,
                              default=lambda x: x.isoformat() if isinstance(x, (date, datetime)) else None)
                              + "\n"
                          )
                          count += 1

              except Exception as exc:
                  raise CidCriticalError(f"Unhandled exception in store_to_temp", exc)

              logger.info(f"Stored {count} record(s) in temp file")
              return count

          def upload_to_s3(account, params, region=None):
              """ Moves the processed API data from the temp file to the designated S3 bucket """
              logger.debug(f"Entering upload_to_s3 for bucket {params.bucket}")
              try:
                  key = get_s3_key(account, params, region)
                  boto3.client('s3').upload_file(params.tmp_file, params.bucket, key)
                  logger.info(f"Data stored to s3://{params.bucket}/{key}")
                  return True

              except Exception as exc:
                  raise CidCriticalError("Exception in upload_to_s3", exc)

          def get_client_with_role(role_name, account_id, service, region, params):
              """ Assumes the designated data gathering read-only role and instantiates a boto3 client with it """
              logger.debug(f"Entering get_client_with_role to get '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'")
              try:
                  credentials = boto3.client('sts').assume_role(
                      RoleArn=f"arn:aws:iam::{account_id}:role/{role_name}",
                      RoleSessionName="data_collection"
                  )['Credentials']
                  logger.debug("Successfully assumed role, now getting client")
                  client = boto3.client(
                      service,
                      region_name = region,
                      aws_access_key_id = credentials['AccessKeyId'],
                      aws_secret_access_key = credentials['SecretAccessKey'],
                      aws_session_token = credentials['SessionToken'],
                      config = params.boto_config
                  )
                  logger.info(f"Successfully created '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'")
                  return client

              except Exception as exc:
                  raise ClientAccessError(exc, role_name, account_id, service, region)

          # Helper classes
          class Account():
              def __init__(self, account_json: dict):
                  try:
                      self.account_id = account_json["account_id"]
                      self.account_name = account_json["account_name"]
                      self.payer_id = account_json["payer_id"]
                  except KeyError:
                      raise CidCriticalError(f"Invalid account data passed {account_json}")

          class CidError(Exception):
              def __init__(self, message="", exc=None):
                  try:
                      message = f"({type(exc).__name__}) exception. {message}" if exc else message
                      if type(self) == CidNonCriticalError.__class__:
                          logger.warning(message)
                      else:
                          logger.error(message)
                      super().__init__(message)
                  except Exception as exc:
                      pass
          class CidNonCriticalError(CidError):
              def __init__(self, message="", exc=None):
                  super().__init__(message, exc)
          class CidCriticalError(CidError):
              def __init__(self, message="", exc=None):
                  super().__init__(message, exc)
          class ClientAccessError(Exception):
              def __init__(self, exc, role_name, account_id, service, region):
                  message = f"({type(exc).__name__}) exception: '{exc}' when getting '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'"
                  logger.warning(message)
                  super().__init__(message)
      Handler: 'index.lambda_handler'
      MemorySize: 2688
      Timeout: 300
      Role: !GetAtt LambdaRole.Arn
      Environment:
        Variables:
          LOG_LEVEL: 'INFO'
          BUCKET_NAME: !Ref DestinationBucket
          MODULE_NAME: !Ref CFDataName
          ROLE_NAME: !Ref MultiAccountRoleName
          DAYS: !Ref DAYS
          REGIONS: !Ref RegionsInScope
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"

  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Sub '${ResourcePrefix}${CFDataName}-Crawler'
      Role: !Ref GlueRoleARN
      DatabaseName: !Ref DatabaseName
      Targets:
        S3Targets:
          - Path: !Sub "s3://${DestinationBucket}/${CFDataName}/rds-usage-data/"
      Configuration: "{\"Version\":1.0,\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"

  ModuleStepFunction:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub '${ResourcePrefix}${CFDataName}-StateMachine'
      StateMachineType: STANDARD
      RoleArn: !Ref StepFunctionExecutionRoleARN
      DefinitionString: !Ref StepFunctionTemplate
      DefinitionSubstitutions:
        AccountCollectorLambdaARN: !Ref AccountCollectorLambdaARN
        ModuleLambdaARN: !GetAtt LambdaFunction.Arn
        Crawlers: !Sub '["${ResourcePrefix}${CFDataName}-Crawler"]'
        CollectionType: "LINKED"
        Params: ''
        Module: !Ref CFDataName
        DeployRegion: !Ref AWS::Region
        Account: !Ref AWS::AccountId
        Prefix: !Ref ResourcePrefix

  ModuleRefreshSchedule:
    Type: 'AWS::Scheduler::Schedule'
    Properties:
      Description: !Sub 'Scheduler for the ODC ${CFDataName} module'
      Name: !Sub '${ResourcePrefix}${CFDataName}-RefreshSchedule'
      ScheduleExpression: !Ref Schedule
      State: ENABLED
      FlexibleTimeWindow:
        MaximumWindowInMinutes: 30
        Mode: 'FLEXIBLE'
      Target:
        Arn: !GetAtt ModuleStepFunction.Arn
        RoleArn: !Ref SchedulerExecutionRoleARN

  LambdaAnalyticsExecutor:
    Type: Custom::LambdaAnalyticsExecutor
    Properties:
      ServiceToken: !Ref LambdaAnalyticsARN
      Name: !Ref CFDataName

  AthenaQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Provides a summary view of the lambda data
      Name: rds-usage-summary-view
      QueryString: !Sub |
        SELECT dbinstanceidentifier, 'memory' as Metric, memory.timestamp, memory.maximum
        FROM ${DatabaseName}.rds_usage_data
        cross join unnest(datapoints.FreeableMemory) as t(memory)
        union
        SELECT dbinstanceidentifier, 'cpu' as Metric, cpu.timestamp, cpu.maximum
        FROM ${DatabaseName}.rds_usage_data
        cross join unnest(datapoints.CPUUtilization) as t(cpu)
        union
        SELECT dbinstanceidentifier, 'NetworkReceiveThroughput' as Metric, NetworkReceiveThroughput.timestamp, NetworkReceiveThroughput.maximum
        FROM ${DatabaseName}.rds_usage_data
        cross join unnest(datapoints.NetworkReceiveThroughput) as t(NetworkReceiveThroughput)
        union
        SELECT dbinstanceidentifier, 'NetworkTransmitThroughput' as Metric, NetworkTransmitThroughput.timestamp, NetworkTransmitThroughput.maximum
        FROM ${DatabaseName}.rds_usage_data
        cross join unnest(datapoints.CPUUtilization) as t(NetworkTransmitThroughput)
        union
        SELECT dbinstanceidentifier, 'ReadIOPS' as Metric, ReadIOPS.timestamp, ReadIOPS.maximum
        FROM ${DatabaseName}.rds_usage_data
        cross join unnest(datapoints.ReadIOPS) as t(ReadIOPS)
        union
        SELECT dbinstanceidentifier, 'WriteIOPS' as Metric, WriteIOPS.timestamp, WriteIOPS.maximum
        FROM ${DatabaseName}.rds_usage_data
        cross join unnest(datapoints.WriteIOPS) as t(WriteIOPS)
        union
        SELECT dbinstanceidentifier, 'FreeStorageSpace' as Metric, FreeStorageSpace.timestamp, FreeStorageSpace.maximum
        FROM ${DatabaseName}.rds_usage_data
        cross join unnest(datapoints.FreeStorageSpace) as t(FreeStorageSpace)

  GravitonMappingTable:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Provides a summary view of the lambda data
      Name: graviton_mapping
      QueryString: !Sub |
        CREATE EXTERNAL TABLE `rds_graviton_mapping`(
          `dbtype` varchar(255),
          `databaseengine` varchar(255),
          `instancetype` varchar(255),
          `graviton_instancetype` varchar(255))
          ROW FORMAT DELIMITED
          FIELDS TERMINATED BY ','
        STORED AS INPUTFORMAT
          'org.apache.hadoop.mapred.TextInputFormat'
        OUTPUTFORMAT
          'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
        LOCATION
          's3://${DestinationBucket}/pricing/graviton'
        TBLPROPERTIES (
          'classification'='csv',
          'transient_lastDdlTime'='1666191659')

  GravitonAthenaQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Provides a view of modernization opportunities for Graviton
      Name: rds-usage-rds-graviton
      QueryString: |
        WITH
        raw_data AS (
          SELECT dbinstanceidentifier,
            dbinstanceclass,
            concat(
              split_part(dbinstanceclass, '.', 1),
              '.',
              split_part(dbinstanceclass, '.', 2)
            ) instance_family,
            availabilityzone,
            rtrim(availabilityzone, 'abcdef') region,
            (CASE
                WHEN (multiaz = false) THEN 'Single-AZ'
                WHEN (multiaz = true) THEN 'Multi-AZ' ELSE 'Error check deploymentoption'
              END
            ) deploymentoption,
            (CASE
                WHEN (engine LIKE 'aurora%') THEN 'Aurora' ELSE 'AmazonRDS'
              END
            ) rds_type,
            engine,
            (CASE
                WHEN (engine LIKE '%postgres%') THEN 'PostgreSQL'
                WHEN (engine LIKE '%mysql%') THEN 'MySQL'
                WHEN (engine LIKE '%mariadb%') THEN 'MariaDB'
                WHEN (engine LIKE '%oracle%') THEN 'Oracle'
                WHEN (engine LIKE '%docdb%') THEN 'DocDB'
                WHEN (engine LIKE '%sqlserver%') THEN 'SQL Server'
                WHEN (engine LIKE 'aurora 5.6.10a') THEN 'aurora 5.6.10a'
                WHEN (engine LIKE '%neptune%') THEN 'Neptune' ELSE 'Check DB Engine'
              END) db_engine,
            (CASE
                WHEN (engineversion LIKE '%aurora%') THEN concat(
                  split_part(substr(engineversion, 18, 6), '.', 1),
                  '.',
                  split_part(substr(engineversion, 18, 6), '.', 2),
                  '.',
                  split_part(substr(engineversion, 18, 6), '.', 3)
                ) ELSE engineversion
              END) engineversion,
            (CASE
                WHEN (engineversion LIKE '%aurora%') THEN split_part(substr(engineversion, 18, 6), '.', 1) ELSE split_part(engineversion, '.', 1)
              END) major,
            (CASE
                WHEN (engineversion LIKE '%aurora%') THEN split_part(substr(engineversion, 18, 6), '.', 2) ELSE split_part(engineversion, '.', 2)
              END) minor,
            (CASE
                WHEN (engineversion LIKE '%aurora%') THEN split_part(substr(engineversion, 18, 6), '.', 3) ELSE split_part(engineversion, '.', 3)
              END) fix,
            accountid,
            year,
            month,
            CAST("concat"("year", '-', "month", '-01') AS date) "billing_period"
          FROM rds_usage_data),
        rds_pricing AS (
          SELECT (CASE
                WHEN ("database engine" LIKE 'Aurora%') THEN 'Aurora'
                WHEN ("database engine" = 'Any') THEN 'Any' ELSE 'AmazonRDS'
              END
            ) rds_type,
            (CASE
                WHEN ("database engine" LIKE 'Aurora%') THEN split_part("database engine", ' ', 2) ELSE "database engine"
              END
            ) db_engine,
            "deployment option",
            location,
            "instance type",
            (CASE
                WHEN (priceperunit is null) THEN 0E0 ELSE CAST(priceperunit AS decimal(18,2))
              END
            ) priceperunit,
            unit
          FROM optimization_data.pricing_rds_data
          WHERE ((("location type" = 'AWS Region')AND (purchaseoption = ''))AND ("product family" = 'Database Instance'))),
        graviton_mapping AS (
          SELECT raw_data.dbinstanceidentifier,
            raw_data.rds_type,
            raw_data.db_engine,
            raw_data.dbinstanceclass,
            (CASE
                WHEN (raw_data.dbinstanceclass LIKE '%g.%') THEN 'Already Graviton'
                WHEN (raw_data.dbinstanceclass LIKE '%gd.%') THEN 'Already Graviton'
                WHEN (raw_data.dbinstanceclass LIKE '%serverless%') THEN 'Ineligible' ELSE (
                  CASE
                    WHEN (raw_data.rds_type = 'AmazonRDS') THEN (
                      CASE
                        WHEN (raw_data.db_engine = 'MySQL') THEN (
                          CASE
                            WHEN (CAST(raw_data.major AS integer) > 8) THEN 'Eligible'
                            WHEN (
                              (CAST(raw_data.major AS integer) = 8)
                              AND (CAST(raw_data.minor AS integer) > 0)
                            ) THEN 'Eligible'
                            WHEN (
                              (
                                (CAST(raw_data.major AS integer) = 8)
                                AND (CAST(raw_data.minor AS integer) = 0)
                              )
                              AND (CAST(raw_data.fix AS integer) >= 17)
                            ) THEN 'Eligible' ELSE 'Requires Update'
                          END
                        )
                        WHEN (raw_data.db_engine = 'PostgreSQL') THEN (
                          CASE
                            WHEN (CAST(raw_data.major AS integer) > 12) THEN 'Eligible'
                            WHEN (
                              (CAST(raw_data.major AS integer) = 12)
                              AND (CAST(raw_data.minor AS integer) >= 3)
                            ) THEN 'Eligible' ELSE 'Requires Update'
                          END
                        )
                        WHEN (raw_data.db_engine = 'MariaDB') THEN (
                          CASE
                            WHEN (CAST(raw_data.major AS integer) > 10) THEN 'Eligible'
                            WHEN (
                              (CAST(raw_data.major AS integer) = 10)
                              AND (CAST(raw_data.minor AS integer) > 4)
                            ) THEN 'Eligible'
                            WHEN (
                              (
                                (CAST(raw_data.major AS integer) = 10)
                                AND (CAST(raw_data.minor AS integer) = 4)
                              )
                              AND (CAST(raw_data.fix AS integer) >= 13)
                            ) THEN 'Eligible' ELSE 'Requires Update'
                          END
                        ) ELSE 'Ineligible'
                      END
                    )
                    WHEN (raw_data.rds_type = 'Aurora') THEN (
                      CASE
                        WHEN (raw_data.db_engine = 'MySQL') THEN (
                          CASE
                            WHEN (CAST(raw_data.major AS integer) > 2) THEN 'Eligible'
                            WHEN (
                              (CAST(raw_data.major AS integer) = 2)
                              AND (CAST(raw_data.minor AS integer) > 9)
                            ) THEN 'Eligible'
                            WHEN (
                              (
                                (CAST(raw_data.major AS integer) = 2)
                                AND (CAST(raw_data.minor AS integer) = 9)
                              )
                              AND (CAST(raw_data.fix AS integer) >= 2)
                            ) THEN 'Eligible' ELSE 'Ineligible'
                          END
                        )
                        WHEN (raw_data.db_engine = 'PostgreSQL') THEN (
                          CASE
                            WHEN (CAST(raw_data.major AS integer) > 12) THEN 'Eligible'
                            WHEN (
                              (CAST(raw_data.major AS integer) = 12)
                              AND (CAST(raw_data.minor AS integer) >= 4)
                            ) THEN 'Eligible' ELSE 'Ineligible'
                          END
                        ) ELSE 'Ineligible'
                      END
                    )
                  END
                )
              END
            ) graviton_eligible,
            rds_graviton_mapping.graviton_instancetype,
            raw_data.region,
            pricing_regionnames_data.regionname,
            raw_data.deploymentoption,
            raw_data.engineversion,
            raw_data.accountid,
            raw_data.year,
            raw_data.month,
            raw_data.billing_period
          FROM (
              (
                raw_data
                LEFT JOIN optimization_data.pricing_regionnames_data ON (raw_data.region = pricing_regionnames_data.region)
              )
              LEFT JOIN optimization_data.rds_graviton_mapping ON (
                (
                  (raw_data.rds_type = rds_graviton_mapping.dbtype)
                  AND (
                    raw_data.db_engine = rds_graviton_mapping.databaseengine
                  )
                )
                AND (
                  raw_data.dbinstanceclass = rds_graviton_mapping.instancetype
                )
              )
            )
        )
        SELECT graviton_mapping.*,
          PL1.priceperunit existing_unit_price,
          (PL1.priceperunit * 720) existing_monthly_price,
          PL2.priceperunit graviton_unit_price,
          (PL2.priceperunit * 720) graviton_montlhy_price,
          round(
            (
              (PL1.priceperunit * 720) - (PL2.priceperunit * 720)
            ),
            2
          ) monthly_savings,
          round(
            (
              (
                (PL1.priceperunit * 720) - (PL2.priceperunit * 720)
              ) * 12
            ),
            2
          ) estimated_annual_savings,
          round((1 - (PL2.priceperunit / PL1.priceperunit)), 2) percentage_savings
        FROM (
            (
              graviton_mapping
              LEFT JOIN rds_pricing PL1 ON (
                (
                  (
                    (
                      (graviton_mapping.rds_type = PL1.rds_type)
                      AND (graviton_mapping.db_engine = PL1.db_engine)
                    )
                    AND (
                      graviton_mapping.deploymentoption = PL1."deployment option"
                    )
                  )
                  AND (graviton_mapping.regionname = PL1.location)
                )
                AND (
                  graviton_mapping.dbinstanceclass = PL1."instance type"
                )
              )
            )
            LEFT JOIN rds_pricing PL2 ON (
              (
                (
                  (
                    (
                      graviton_mapping.graviton_instancetype = PL2."instance type"
                    )
                    AND (graviton_mapping.regionname = PL2.location)
                  )
                  AND (graviton_mapping.rds_type = PL2.rds_type)
                )
                AND (graviton_mapping.db_engine = PL2.db_engine)
              )
              AND (
                graviton_mapping.deploymentoption = PL2."deployment option"
              )
            )
          )


