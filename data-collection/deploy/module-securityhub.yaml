AWSTemplateFormatVersion: "2010-09-09"
Description: Retrieves AWS Security Hub details across AWS organization
Parameters:
  DatabaseName:
    Type: String
    Description: Name of the Athena database to be created to hold lambda information
    Default: optimization_data
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket that exists or needs to be created to hold costanomaly information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: securityhub
  ResourcePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  LambdaAnalyticsARN:
    Type: String
    Description: Arn of lambda for Analytics

Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${ResourcePrefix}${CFDataName}-LambdaRole"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Path: /
      Policies:
        - PolicyName: "S3-Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:PutObject"
                  - "s3:GetObject"
                  - "s3:PutObjectAcl"
                Resource:
                  - !Sub "arn:${AWS::Partition}:s3:::${DestinationBucket}/*"
              - Effect: "Allow"
                Action:
                  - "s3:ListBucket"
                Resource:
                  - !Sub "arn:${AWS::Partition}:s3:::${DestinationBucket}"

    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  EventBridgeInvokeFirehoseRole:
    UpdateReplacePolicy: "Delete"
    Type: "AWS::IAM::Role"
    DeletionPolicy: "Delete"
    Properties:
      Path: "/service-role/"
      MaxSessionDuration: 3600
      RoleName: !Sub "${ResourcePrefix}${CFDataName}-EventBridgeInvokeFirehose"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Action: "sts:AssumeRole"
          Effect: "Allow"
          Principal:
            Service: "events.amazonaws.com"
      Policies:
        - PolicyName: "Firehose-WriteAccess"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "firehose:PutRecord"
                  - "firehose:PutRecordBatch"
                Resource:
                  - !GetAtt SecHubEventsFirehoseDeliveryStream.Arn
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  KinesisFirehoseRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${ResourcePrefix}${CFDataName}-KinesisFireHoseRole"
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Action: sts:AssumeRole
            Effect: Allow
            Principal:
              Service: firehose.amazonaws.com
            Condition:
              StringEquals:
                'sts:ExternalId': !Ref 'AWS::AccountId'
      Path: /
      Policies:
        - PolicyName: "SecurityHub-S3-Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                - "s3:PutObject"
                - "s3:GetObject"
                - "s3:ListBucketMultipartUploads"
                - "s3:AbortMultipartUpload"
                Resource:
                - !Sub "arn:${AWS::Partition}:s3:::${DestinationBucket}/*"
              - Effect: "Allow"
                Action:
                  - "s3:ListBucket"
                  - "s3:GetBucketLocation"
                Resource:
                - !Sub "arn:${AWS::Partition}:s3:::${DestinationBucket}/*"
              - Effect: "Allow"
                Action:
                  - "lambda:InvokeFunction"
                  - "lambda:GetFunctionConfiguration"
                Resource:
                - !GetAtt LambdaFunction.Arn
              - Effect: "Allow"
                Action:
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource: !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/*"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  EventRuleSecurityHubKinesisIntegration:
    Type: "AWS::Events::Rule"
    Properties:
      EventPattern:
        detail-type:
        - "Security Hub Findings - Imported"
        source:
        - "aws.securityhub" # only AWS can send events with source aws.*
        - "cid.test" # uncomment for testing
      Targets:
      - Arn: !GetAtt SecHubEventsFirehoseDeliveryStream.Arn
        RoleArn: !GetAtt EventBridgeInvokeFirehoseRole.Arn
        Id: "Firehose"
      State: "ENABLED"
      Name: !Sub "${ResourcePrefix}${CFDataName}-SecurityHubToFirehose"
  SecHubstatemachineRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Action: sts:AssumeRole
            Effect: Allow
            Principal:
              Service: !Join
                - ''
                - - states.
                  - !Ref AWS::Region
                  - .amazonaws.com
        Version: '2012-10-17'
  SecHubstatemachineRoleDefaultPolicy:
    Type: AWS::IAM::Policy
    Properties:
      PolicyDocument:
        Statement:
          - Action: lambda:InvokeFunction
            Effect: Allow
            Resource:
              - - !Sub "${SecHubBackfillFunction.Arn}/:*"
          - Action: ssm:PutParameter
            Effect: Allow
            Resource: !Join
              - ''
              - - 'arn:'
                - !Ref AWS::Partition
                - ':ssm:'
                - !Ref AWS::Region
                - ':'
                - !Ref AWS::AccountId
                - ':parameter'
                - !Ref SecHubcountParameter
        Version: '2012-10-17'
      PolicyName: SecHubstatemachineRole
      Roles:
        - !Ref SecHubstatemachineRole
  sechubstatemachine4A890B2A:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      RoleArn: !GetAtt SecHubstatemachineRole.Arn
      DefinitionString: !Join
        - ''
        - - '{"StartAt":"GetSecurityHubFindings","States":{"GetSecurityHubFindings":{"Next":"Check for NextToken in Security Hub Findings response.","Retry":[{"ErrorEquals":["Lambda.ServiceException","Lambda.AWSLambdaException","Lambda.SdkClientException"],"IntervalSeconds":2,"MaxAttempts":6,"BackoffRate":2},{"ErrorEquals":["States.ALL"],"IntervalSeconds":10,"MaxAttempts":5,"BackoffRate":2}],"Type":"Task","InputPath":"$","OutputPath":"$","Resource":"arn:'
          - !Ref AWS::Partition
          - ':states:::lambda:invoke","Parameters":{"FunctionName":"'
          - !GetAtt SecHubBackfillFunction.Arn
          - '","Payload.$":"$"}},"Check for NextToken in Security Hub Findings response.":{"Type":"Choice","Choices":[{"Variable":"$.Payload.NextToken","IsNull":false,"Next":"GetSecurityHubFindings"}],"Default":"Security Hub Export Succeded"},"Security Hub Export Succeded":{"Type":"Succeed"}}}'
      StateMachineName: sec_hub_finding_export
  EventRuleSechubKinesisIntegration:
    Type: "AWS::Events::Rule"
    Properties:
      EventPattern:
        detail-type:
        - "Security Hub Findings - Imported"
        source:
        - "aws.securityhub"
      Targets:
      - Arn: !GetAtt SecHubEventsFirehoseDeliveryStream.Arn
        RoleArn: !GetAtt IAMRoleAmazonEventBridgeInvokeFirehose.Arn
        Id: "FirehoseDeliveryStream"
      State: "ENABLED"
      Name: "${ResourcePrefix}${CFDataName}-SecHubEvents"

  SecHubcountParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Type: String
      Value: '0'
      Description: The Count for Security Hub findings for Backfill purpose.
      Name: /sechubexport/count

  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ResourcePrefix}${CFDataName}-transformation-Lambda'
      Description: "Lambda function to transform Security Hub events"
      Runtime: python3.12
      Architectures: [x86_64]
      Code:
        ZipFile: |
          import os
          import re
          import json
          import logging
          import base64

          def rename_keys(payload):
              """Recursively rename all special characters in keys to '_'. """
              if isinstance(payload, dict):
                  renamed_payload = {}
                  for key, value in payload.items():
                      renamed_key = re.sub(r'\W+', '_', key)
                      renamed_value = rename_keys(value)
                      renamed_payload[renamed_key] = renamed_value
                  return renamed_payload
              elif isinstance(payload, list):
                  renamed_list = []
                  for item in payload:
                      renamed_item = rename_keys(item)
                      renamed_list.append(renamed_item)
                  return renamed_list
              elif isinstance(payload, (str, int, float, bool, type(None))):
                  return payload
              else:
                  if hasattr(payload, "__dict__"):
                      renamed_payload = {}
                      for key, value in payload.__dict__.items():
                          renamed_key = re.sub(r'\W+', '_', key)
                          renamed_value = rename_keys(value)
                          renamed_payload[renamed_key] = renamed_value
                      return renamed_payload
                  else:
                      return payload

          def lambda_handler(event, context):
              logger = logging.getLogger()
              payload = event['records']
              transformed_records = []

              for record in payload:
                  data = record['data']
                  decoded_data = base64.b64decode(data).decode('utf-8')
                  print(f"Input record JSON: {decoded_data}")
                  renamed_payload = rename_keys(json.loads(decoded_data))
                  flattened_data = json.dumps(renamed_payload, separators=(',', ':'))
                  transformed_record = {
                      'recordId': record['recordId'],
                      'result': 'Ok',
                      'data': base64.b64encode(flattened_data.encode('utf-8')).decode('utf-8')
                  }
                  print(f"Transformed record: {transformed_record}")
                  transformed_records.append(transformed_record)
              return {'records': transformed_records}
      Handler: "index.lambda_handler"
      MemorySize: 2688
      Timeout: 900
      Role: !GetAtt LambdaRole.Arn
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"
  SecHubBackfillFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ResourcePrefix}${CFDataName}-BackfillLambda'
      Description: !Sub "Lambda function to Backfill SecurityHub Active & Archived findings for last 90days ${CFDataName}"
      Runtime: python3.10
      Architectures: [x86_64]
      Environment:
        Variables:
          REGION: !Ref AWS::Region
          account: !Ref AWS::AccountId
          S3_BUCKET: !GetAtt DestinationBucketARN.Arn
          SSM_PARAMETER_COUNT: !Ref sechubcountparameterD48E37A7
      Handler: get_sh_finding.lambda_handler
      MemorySize: 1024
      ReservedConcurrentExecutions: 100
      Timeout: 900
      Code:
        ZipFile: |
          import boto3
          import json
          import uuid
          from botocore.exceptions import ClientError
          import datetime
          import logging
          import os
          import time
          import gzip
          from io import BytesIO

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          S3_BUCKET = os.environ['S3_BUCKET']
          account = os.environ['account']
          REGION = os.environ['REGION']

          SSM_PARAMETER_COUNT = os.environ['SSM_PARAMETER_COUNT']

          sechub = boto3.client('securityhub')
          s3 = boto3.resource('s3')
          ssm = boto3.client('ssm')

          def create_filter():
              # converted_string_date = datetime.datetime.strptime(date_filter, '%Y-%m-%dT%H:%M:%S.%fZ')
              # day_counter = 90
              # updatedat_end = str(converted_string_date.isoformat()[:-3]+'Z')
              # updatedat_start = str((converted_string_date - datetime.timedelta(days=day_counter)).isoformat()[:-3]+'Z')
              # logger.info("Creating finding filter to get findings from {} to {}...".format(updatedat_start,updatedat_end))

              finding_filter = {
                  # 'UpdatedAt': [
                  #     {
                  #         'Start': updatedat_start,
                  #         'End': updatedat_end
                  #     },
                  # ],
              }
              return finding_filter

          def get_findings(sechub, finding_filter, next_token):
              max_iterator = 50
              results = []
              logger.info("Running export for Security Hub findings...")
              for x in range(0, max_iterator, 1):
                  try:
                      response = sechub.get_findings(
                          Filters=finding_filter,
                          NextToken=next_token,
                          MaxResults=100
                      )
                      results.extend(response["Findings"])
                      if "NextToken" in response:
                          next_token = response['NextToken']
                      else:
                          logger.info("NextToken not found. Ending Security Hub finding export.")
                          next_token = None
                          break
                  except ClientError as error_handle:
                      if error_handle.response['Error']['Code'] == 'TooManyRequestsException':
                          time.sleep(5)
                          logger.warning('Catching Security Hub API Throttle...')
                          next_token = response['NextToken']
                  except Exception as exception_handle:
                      logger.error(exception_handle)
                      next_token = response['NextToken']
              logger.info("Consolidating {} findings...".format(len(results)))
              consolidated_results = json.dumps({
                  "version": "0",
                  "id": str(uuid.uuid4()),
                  "detail_type": "sechub-backfill",
                  "source": "security-hub-backfill_data",
                  "account": account,
                  "time": datetime.datetime.now().isoformat(),
                  "region": REGION,
                  "resources": [
                      "backfill_resource"
                  ],
                  "detail": {
                      "findings": results
                  }
              })
              return next_token, results, consolidated_results

          def sechub_count_value(results):
              logger.info("Adding {} Security Hub findings to export count...".format(len(results)))
              try:
                  existing_value = ssm.get_parameter(
                      Name=SSM_PARAMETER_COUNT
                  )
                  existing_value['Parameter']['Value']
                  sechub_count = (int(existing_value['Parameter']['Value'])) + len(results)
                  response = ssm.put_parameter(
                      Name=SSM_PARAMETER_COUNT,
                      Value=str(sechub_count),
                      Overwrite=True,
                  )
                  logger.info("Current Security Hub export count is {}.".format(str(sechub_count)))
              except ClientError as error_handle:
                  logger.error(error_handle)
              return sechub_count

          def put_obj_to_s3(results, consolidated_results):
              key = f"sechub_events/managementid={account}/{datetime.datetime.now().strftime('%Y/%m/%d/%H')}/security-hub-finding-backfill{uuid.uuid4()}.gz"
              try:
                  logger.info("Exporting {} findings to s3://{}/{}".format(len(results), S3_BUCKET, key))

                  # Compress the JSON data with GZip
                  compressed_data = BytesIO()
                  with gzip.GzipFile(fileobj=compressed_data, mode='w') as gz_file:
                      for result in results:
                          json_obj = json.dumps({
                              "version": "0",
                              "id": str(uuid.uuid4()),
                              "detail_type": "sechub-backfill",
                              "source": "security-hub-backfill_data",
                              "account": account,
                              "time": datetime.datetime.now().isoformat(),
                              "region": REGION,
                              "resources": [
                                  "backfill_resource"
                              ],
                              "detail": {
                                  "findings": [result]
                              }
                          })
                          gz_file.write(json_obj.encode() + b'\n')
                  compressed_data.seek(0)

                  response = s3.Bucket(S3_BUCKET).put_object(
                      Key=key,
                      Body=compressed_data
                  )
                  logger.info("Successfully exported {} findings to s3://{}/{}".format(len(results), S3_BUCKET, key))
              except ClientError as error_handle:
                  if error_handle.response['Error']['Code'] == 'ConnectTimeoutError':
                      time.sleep(5)
                      logger.warning('Catching Connection Timeout Error...')
              except Exception as exception_handle:
                  logger.error(exception_handle)


          def lambda_handler(event, context):
              print(event)
              if 'Payload' in event:
                  next_token = event['Payload']['NextToken']
          #        date_filter = event['Payload']['StartDate']
                  logger.info("NextToken {} detected for Security Hub findings.".format(next_token))
              else:
                  next_token = ''
          #        date_filter=event['StartDate']
                  logger.info("NextToken not detected for Security Hub findings.")
              finding_filter = create_filter()
              runs = 25
              for a in range(0, runs, 1):
                  if (next_token is not None):
                      next_token, results, consolidated_results = get_findings(sechub, finding_filter, next_token)
                      put_obj_to_s3(results, consolidated_results)
                      sechub_count = sechub_count_value(results)
                  else:
                      logger.info("NextToken not found... Ending Security Hub finding export.")
                      break
              return {
                  'NextToken': next_token,
                  'SecHubCount': sechub_count,
          #        'StartDate': date_filter
              }

    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"

  FirehoseLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/kinesis/${ResourcePrefix}${CFDataName}-detail-stream"
      RetentionInDays: 60

  SecHubEventsFirehoseDeliveryStream:
      Type: AWS::KinesisFirehose::DeliveryStream
      Properties:
        DeliveryStreamName: !Sub '${ResourcePrefix}${CFDataName}-detail-stream'
        DeliveryStreamType: DirectPut
        DeliveryStreamEncryptionConfigurationInput:
          KeyType: AWS_OWNED_CMK
        ExtendedS3DestinationConfiguration:
          BucketARN: !Sub "arn:${AWS::Partition}:s3:::${DestinationBucket}"
          Prefix: !Sub "${CFDataName}/securityhub_events/!{timestamp:yyyy}/!{timestamp:MM}/!{timestamp:dd}/"
          ErrorOutputPrefix: !Sub "${CFDataName}/securityhub_errors/!{timestamp:yyyy/MM/}/!{firehose:error-output-type}"
          RoleARN: !GetAtt KinesisFirehoseRole.Arn
          CloudWatchLoggingOptions:
            Enabled: true
            LogGroupName: !Ref FirehoseLogGroup
            LogStreamName: SecHubEventsFirehoseDeliveryStreamLog
          BufferingHints:
            IntervalInSeconds: 900
            SizeInMBs: 30
          CompressionFormat: "GZIP"
          ProcessingConfiguration:
            Enabled: true
            Processors:
            - Type: Lambda
              Parameters:
              - ParameterName: LambdaArn
                ParameterValue: !GetAtt LambdaFunction.Arn
              - ParameterName: BufferIntervalInSeconds
                ParameterValue: 600
              - ParameterName: BufferSizeInMBs
                ParameterValue: 3

  TableSecurityHubEvents:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref "AWS::AccountId"
      DatabaseName: !Ref DatabaseName
      TableInput:
        Name: securityhub_events
        TableType: EXTERNAL_TABLE
        PartitionKeys:
        - { Name: datehour,    Type: string }
        StorageDescriptor:
          Columns:
          - { Name: version,     Type: string }
          - { Name: id,          Type: string }
          - { Name: detail_type, Type: string }
          - { Name: source,      Type: string }
          - { Name: account,     Type: string }
          - { Name: time,        Type: string }
          - { Name: region,      Type: string }
          - { Name: resources,   Type: array<string> }
          - { Name: detail,      Type: string }
          InputFormat: org.apache.hadoop.mapred.TextInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
          Location: !Sub "s3://${DestinationBucket}/${CFDataName}/securityhub_events/"
          Parameters: {}
          SerdeInfo:
            SerializationLibrary: org.openx.data.jsonserde.JsonSerDe
            Parameters:
              serialization.format: '1'
        Parameters:
          EXTERNAL: 'TRUE'
          projection.datehour.format: yyyy/MM/dd
          projection.datehour.interval: '1'
          projection.datehour.interval.unit: DAYS
          projection.datehour.range: 2024/07/01,NOW
          projection.datehour.type: date
          projection.enabled: 'true'
          storage.location.template: !Sub "s3://${DestinationBucket}/${CFDataName}/securityhub_events/${!datehour}"

  AnalyticsExecutor:
    Type: Custom::LambdaAnalyticsExecutor
    Properties:
      ServiceToken: !Ref LambdaAnalyticsARN
      Name: !Ref CFDataName