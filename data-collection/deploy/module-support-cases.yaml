AWSTemplateFormatVersion: '2010-09-09'
Description: Retrieves Inventory data for the chosen service
Parameters:
  DatabaseName:
    Type: String
    Description: Name of the Athena database to be created to hold AWS Support Cases information
    Default: optimization_data
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket to be created to hold AWS Support Cases information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  DestinationBucketARN:
    Type: String
    Description: ARN of the S3 Bucket that exists or needs to be created to hold AWS Support Cases information
  MultiAccountRoleName:
    Type: String
    Description: Name of the IAM role deployed in all accounts which can retrieve AWS Support Cases Data.
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: support-cases
  GlueRoleARN:
    Type: String
    Description: Arn for the Glue Crawler role
  Schedule:
    Type: String
    Description: EventBridge Schedule to trigger the data collection
    Default: "rate(14 days)"
  ResourcePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  LambdaAnalyticsARN:
    Type: String
    Description: Arn of lambda for Analytics
  AccountCollectorLambdaARN:
    Type: String
    Description: Arn of the Account Collector Lambda
  CodeBucket:
    Type: String
    Description: Source code bucket
  StepFunctionTemplate:
    Type: String
    Description: S3 key to the JSON template for the StepFunction
  StepFunctionExecutionRoleARN:
    Type: String
    Description: Common role for Step Function execution
  SchedulerExecutionRoleARN:
    Type: String
    Description: Common role for module Scheduler execution

Outputs:
  StepFunctionARN:
    Description: ARN for the module's Step Function
    Value: !GetAtt ModuleStepFunction.Arn

Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${ResourcePrefix}${CFDataName}-LambdaRole"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Path: /
      Policies:
        - PolicyName: "AssumeMultiAccountRole"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: !Sub "arn:aws:iam::*:role/${MultiAccountRoleName}"
        - PolicyName: "S3-Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:PutObject"
                  - "s3:GetObject"
                  - "s3:PutObjectAcl"
                Resource:
                  - !Sub "${DestinationBucketARN}/*"
              - Effect: "Allow"
                Action:
                  - "s3:ListBucket"
                Resource:
                  - !Sub "${DestinationBucketARN}"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28 # Resource found with an explicit name, this disallows updates that require replacement of this resource
            reason: "Need explicit name to identify role actions"

  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ResourcePrefix}${CFDataName}-Lambda'
      Description: !Sub "Lambda function to retrieve ${CFDataName}"
      Runtime: python3.10
      Architectures: [x86_64]
      Code:
        ZipFile: |
          import os
          import json
          import logging
          from datetime import date, timedelta, datetime

          import boto3

          BUCKET = os.environ['BUCKET_NAME']
          ROLE_NAME = os.environ['ROLE_NAME']
          MODULE_NAME = os.environ['PREFIX']
          TMP_FILE = '/tmp/tmp.json'
          REGION = "us-east-1"

          logger = logging.getLogger(__name__)
          logger.setLevel(getattr(logging, os.environ.get('LOG_LEVEL', 'INFO').upper(), logging.INFO))

          def lambda_handler(event, context): #pylint: disable=unused-argument
              logger.info(f"Incoming event: {json.dumps(event)}")
              key = "account"
              if key not in event:
                  logger.error(f"Lambda event parameter '{key}' not defined (fatal) in {MODULE_NAME} module. Please do not trigger this Lambda manually. "
                      f"Find the corresponding {MODULE_NAME} state machine in Step Functions and trigger from there."
                  )
                  raise RuntimeError(f"(MissingParameterError) Lambda event missing '{key}' parameter")

              account = json.loads(event[key])
              main(account, ROLE_NAME, MODULE_NAME, BUCKET)

              return {
                  'statusCode': 200
              }

          def calculate_dates(bucket, s3_path):
              end_date = datetime.now().date()
              start_date = datetime.now().date() - timedelta(days=90) #Support Cases are available for last 90days
              # Check the create time of objects in the S3 bucket
              paginator = boto3.client('s3').get_paginator('list_objects_v2')
              contents = sum( [page.get('Contents', []) for page in paginator.paginate(Bucket=bucket, Prefix=s3_path)], [])
              last_modified_date = get_last_modified_date(contents)
              if last_modified_date and last_modified_date >= start_date:
                  start_date = last_modified_date
              return start_date, end_date

          def get_last_modified_date(contents):
              last_modified_dates = [obj['LastModified'].date() for obj in contents]
              last_modified_dates_within_90_days = [date for date in last_modified_dates if date >= datetime.now().date() - timedelta(days=90)]
              if last_modified_dates_within_90_days:
                  return max(last_modified_dates_within_90_days)
              return None

          def get_client_with_role(role_name, account_id, service, region):
              logger.debug(f"Attempting to get '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'")
              credentials = boto3.client('sts').assume_role(
                  RoleArn=f"arn:aws:iam::{account_id}:role/{role_name}",
                  RoleSessionName="data_collection"
              )['Credentials']
              logger.debug("Successfully assumed role, now getting client")
              client = boto3.client(
                  service,
                  region_name = region,
                  aws_access_key_id=credentials['AccessKeyId'],
                  aws_secret_access_key=credentials['SecretAccessKey'],
                  aws_session_token=credentials['SessionToken'],
              )
              logger.debug(f"Successfully created '{service}' client with role '{role_name}' from account '{account_id}' in region '{region}'")
              return client

          def get_value_by_path(data, path, default=None):
              logger.debug(f"Traversing for path {path}")
              keys = path.split("/")
              current = data
              for key in keys:
                  if isinstance(current, dict) and key in current:
                      current = current.get(key, default)
                  elif isinstance(current, list) and key.isdigit():
                      try:
                          current = current[int(key)]
                      except IndexError:
                          logger.debug(f"Index value {key} within path {path} is not valid in get_value_by_path for data {data}, returning default of {default}")
                          return default
                  else:
                      logger.debug(f"Key value {key} within path {path} is not valid in get_value_by_path for data {data}, returning default of {default}")
                      return default
              return current

          def parse_support_cases_data(record):
              logger.debug(f"Processing record {record}")
              result = {
                  'CaseId': get_value_by_path(record, 'caseId'),
                  'DisplayId': get_value_by_path(record, 'displayId'),
                  'Subject': get_value_by_path(record, 'subject'),
                  'Status': get_value_by_path(record, 'status'),
                  'ServiceCode': get_value_by_path(record, 'serviceCode'),
                  'CategoryCode': get_value_by_path(record, 'categoryCode'),
                  'SeverityCode': get_value_by_path(record, 'severityCode'),
                  'SubmittedBy': get_value_by_path(record, 'submittedBy'),
                  'TimeCreated': get_value_by_path(record, 'timeCreated'),
                  'CCEmailAddresses': get_value_by_path(record, 'ccEmailAddresses'),
                  'Language': get_value_by_path(record, 'language'),
              }
              logger.debug("Processing record complete")
              return result

          def parse_support_cases_communications(record):
              logger.debug(f"Processing record {record}")
              result = {
                  'CaseId': get_value_by_path(record, 'caseId'),
                  'Body': get_value_by_path(record, 'body'),
                  'SubmittedBy': get_value_by_path(record, 'submittedBy'),
                  'TimeCreated': get_value_by_path(record, 'timeCreated'),
                  'AttachmentSet': get_value_by_path(record, 'attachmentSet/0'),
              }
              logger.debug("Processing record complete")
              return result

          def to_json(obj):
              return json.dumps(
                  obj,
                  default=lambda x:
                      x.isoformat() if isinstance(x, (date, datetime)) else None
              )

          def process_support_cases_data(module_name, payer_id, account_id, records, bucket, client):
              for record in records['cases']:
                  data = parse_support_cases_data(record)
                  file_name = f"{data['CaseId']}.json"
                  with open(f"/tmp/{file_name}", "w", encoding='utf-8') as f:
                      f.write(to_json(data))
                  key = datetime.strptime(data["TimeCreated"], '%Y-%m-%dT%H:%M:%S.%fZ').strftime(
                      f"{module_name}/" +
                      f"{module_name}-data/" +
                      f"payer_id={payer_id}/" +
                      f"account_id={account_id}/" +
                      f"year=%Y/month=%m/day=%d/{data['CaseId']}.json"
                  )
                  boto3.client('s3').upload_file(f"/tmp/{file_name}", bucket, key)
                  logger.info(f"Data stored to s3://{bucket}/{key}")
                  logger.info(f"Retrieving communications for {data['CaseId']}...")
                  communications_pages = client.get_paginator('describe_communications').paginate(
                      caseId = data['CaseId'],
                      PaginationConfig = {
                          'PageSize': 100
                      }
                  )
                  for page in communications_pages:
                      if len(page["communications"]) > 0:
                          for comm in page["communications"]:
                              communications = parse_support_cases_communications(comm)
                              logger.info(f"parsed_comms: {communications}")
                              with open(f"/tmp/{file_name}", "w", encoding='utf-8') as f:
                                  f.write(to_json(communications))
                              key = datetime.strptime(communications["TimeCreated"], '%Y-%m-%dT%H:%M:%S.%fZ').strftime(
                                  f"{module_name}/" +
                                  f"{module_name}-communications/" +
                                  f"payer_id={payer_id}/" +
                                  f"account_id={account_id}/" +
                                  f"year=%Y/month=%m/day=%d/hour=%H/min=%M/{communications['CaseId']}.json"
                              )
                              boto3.client('s3').upload_file(f"/tmp/{file_name}", bucket, key)
                  logger.info(f"Processed a total of {len(page['communications'])} support case communications associated with case: {data['CaseId']}")
              logger.info(f"Processed a total of {len(records)} support cases")
              return None

          def main(account, role_name, module_name, bucket):
              start_date, end_date = calculate_dates(bucket, s3_path=f'{module_name}/support-cases-data/')
              logger.info(f'Using start_date={start_date}, end_date={end_date}')

              account_id = account["account_id"]
              payer_id = account["payer_id"]
              client = get_client_with_role(role_name, account_id, region=REGION, service="support")
              case_pages = client.get_paginator('describe_cases').paginate(
                  afterTime = str(start_date),
                  beforeTime = str(end_date),
                  includeCommunications = False,
                  includeResolvedCases = True,
                  PaginationConfig = {
                      'PageSize': 100
                  }
              )
              record_count = 0
              for page in case_pages:
                  if len(page["cases"]) > 0:
                      process_support_cases_data(module_name, payer_id, account_id, page, bucket, client)
                      record_count += len(page["cases"])
              logger.info(f"API results total {record_count}")
      Handler: 'index.lambda_handler'
      MemorySize: 2688
      Timeout: 300
      Role: !GetAtt LambdaRole.Arn
      Environment:
        Variables:
          BUCKET_NAME: !Ref DestinationBucket
          PREFIX: !Ref CFDataName
          ROLE_NAME: !Ref MultiAccountRoleName
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W89 # Lambda functions should be deployed inside a VPC
            reason: "No need for VPC in this case"
          - id: W92 #  Lambda functions should define ReservedConcurrentExecutions to reserve simultaneous executions
            reason: "No need for simultaneous execution"

  LogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaFunction}"
      RetentionInDays: 60

  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Sub '${ResourcePrefix}${CFDataName}-Crawler'
      Role: !Ref GlueRoleARN
      DatabaseName: !Ref DatabaseName
      Targets:
        S3Targets:
          - Path: !Sub "s3://${DestinationBucket}/${CFDataName}/${CFDataName}-data/"
          - Path: !Sub "s3://${DestinationBucket}/${CFDataName}/${CFDataName}-communications/"
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableGroupingPolicy\":\"CombineCompatibleSchemas\"}}"

  ModuleStepFunction:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      StateMachineName: !Sub '${ResourcePrefix}${CFDataName}-StateMachine'
      StateMachineType: STANDARD
      RoleArn: !Ref StepFunctionExecutionRoleARN
      DefinitionS3Location:
        Bucket: !Ref CodeBucket
        Key: !Ref StepFunctionTemplate
      DefinitionSubstitutions:
        AccountCollectorLambdaARN: !Ref AccountCollectorLambdaARN
        ModuleLambdaARN: !GetAtt LambdaFunction.Arn
        Crawlers: !Sub '["${ResourcePrefix}${CFDataName}-Crawler"]'
        CollectionType: "LINKED"
        Params: ''
        Module: !Ref CFDataName
        DeployRegion: !Ref AWS::Region
        Account: !Ref AWS::AccountId
        Prefix: !Ref ResourcePrefix

  ModuleRefreshSchedule:
    Type: 'AWS::Scheduler::Schedule'
    Properties:
      Description: !Sub 'Scheduler for the ODC ${CFDataName} module'
      Name: !Sub '${ResourcePrefix}${CFDataName}-RefreshSchedule'
      ScheduleExpression: !Ref Schedule
      State: ENABLED
      FlexibleTimeWindow:
        MaximumWindowInMinutes: 30
        Mode: 'FLEXIBLE'
      Target:
        Arn: !GetAtt ModuleStepFunction.Arn
        RoleArn: !Ref SchedulerExecutionRoleARN
        Input: !Sub '{"module_lambda":"${LambdaFunction.Arn}","crawlers": ["${ResourcePrefix}${CFDataName}-Crawler"]}'

  AnalyticsExecutor:
    Type: Custom::LambdaAnalyticsExecutor
    Properties:
      ServiceToken: !Ref LambdaAnalyticsARN
      Name: !Ref CFDataName
